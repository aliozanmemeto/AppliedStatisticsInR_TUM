---
title: "Exercise 1"
output: html_document
date: "2024-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sample Statistics

#### Dataset

Go to UCI Machine learning repository and download the data on the white wine quality. This page contains also the background information on the data. In our analysis we will only consider the following variables:

-   volatile.acidity: Volatile acidity

-   residual.sugar: Residual sugar

-   pH: pH level

-   quality: Wine quality in a score between 0 and 10

#### Exercises

1. Read the data into R. Add to the data frame a new binary variable good which is 1 if quality \> 5 and 0 otherwise. We would like to compare volatile.acidity and residual.sugar for good and bad wines.

```{r a, warning=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(ggplot2)
library(ISLR)
library(glmnet)

white.wine.quality <- read.csv("winequality-white.csv", sep = ";")
white.wine.quality <- white.wine.quality %>% select(volatile.acidity, residual.sugar, 
                              pH, quality)

white.wine.quality <- white.wine.quality %>% 
  mutate(good = quality > 5)

head(white.wine.quality)
```

2. First consider variable residual.sugar.

+ Plot histograms of residual.sugar for good and bad wines using different methods available in R to choose the bin width. Comment on the shape of both histograms and differences in distribution, if any.

```{r hist.Sturges}

good.wine <- white.wine.quality[white.wine.quality$good == 1,]
bad.wine <- white.wine.quality[white.wine.quality$good == 0,]

hist(good.wine$residual.sugar,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Residual Sugar of Good Wine with Sturges' Rule",
 xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Residual Sugar of Bad Wine with Sturges' Rule",
 xlab = "Residual Sugar")
```

There are 1640 bad wine and 3258 good wine rows. The distribution of good wine seems to have a long tail on the right. The number of bins for good wine is 14 and it is 12 for bad wine when Sturges formula is used.

```{r hist.ScottsChoice}
hist(good.wine$residual.sugar,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Residual Sugar of Good Wine with Scott's Rule",
     xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Residual Sugar of Bad Wine with Scott's Rule",
     xlab = "Residual Sugar")
```

When Scott's choice is used, the number of breaks remains the same for the bad wine, but it increases significantly for the good wine. This allows us to capture more information, but it also introduces more noise.

```{r hist.FreedmanDiaconis}
hist(good.wine$residual.sugar,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Residual Sugar of Good Wine with Freedman-Diaconis Rule",
     xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Residual Sugar of Bad Wine with Freedman-Diaconis Rule",
     xlab = "Residual Sugar")
```

When Freedman-Diaconis' choice is used, the number of breaks is the same as with Scott's choice. Both Freedman-Diaconis' and Scott's choices take variability into account. However, Freedman-Diaconis' choice utilizes the interquartile range, which can make it more robust to outliers.

+   Calculate the summary statistics for both wine groups, that is: mean, median, standard deviation, interquartile range, minimum and maximum of both samples. Display the results in a table and comment on the differences between both groups, if any.


```{r summary}
white.wine.quality %>%
  group_by(good) %>%
  summarise(mean.rs = mean(residual.sugar),
                        median.rs = median(residual.sugar),
                        sd.rs = sd(residual.sugar),
                        IQR.rs = IQR(residual.sugar),
                        min.rs = min(residual.sugar),
                        max.rs = max(residual.sugar))

```
Considering that both of these variables from the good and bad groups are bounded by 0, the maximum values of residual sugar in good wine seem to be very high, possibly indicating an outlier or a high tail on the right side. Despite this high value, the interquartile range (IQR) of good wine is lower than that of bad wine, supporting the fact that the IQR is a variability measure highly robust to outliers.

Despite the presence of the outlier, we can observe that the mean residual sugar of bad wine is higher than that of good wine. The median also appears to be considerably higher. However, the question arises whether these differences are statistically significant.

+   Generate boxplots for both samples, placing them into one graphic. What do you observe?
```{r boxplot}

p <- ggplot(white.wine.quality, aes(good, residual.sugar))
p + geom_boxplot()

```

We can support the pattern we encountered in the previous question. It is highly likely that the value 65.8 is an outlier. There are a couple more values above the upper whisker. The median of the bad wine group is slightly higher than that of the good wine group. 

+   Generate a QQ-plot to compare two groups. Make sure to choose the same range for both axes. Add a y = x line to the plots. Comment on the results.

```{r qqplot}

qqplot(bad.wine$residual.sugar,good.wine$residual.sugar, main = "QQ-plot: Good vs Bad Wines",
       xlab = "Residual Sugar - Good Wines", ylab = "Residual Sugar - Bad Wines")
abline(0, 1, col = "red")  # Add y = x line
```

We can observe that the residual sugar distribution of good wine is more right-skewed than that of bad wine. Additionally, we can identify those extreme values in the distribution of good wines.

+   Plot the empirical distribution functions of both groups in one graphic. Use different
styles and add a legend. Interpret the results.

```{r ecdf}
ecdf.func <- function(data) { 
  Length <- length(data) 
  sorted <- sort(data) 
  
  ecdf <- rep(0, Length) 
  for (i in 1:Length) { 
    ecdf[i] <- sum(sorted <= sorted[i]) / Length 
  } 
  return(list(sorted = sorted, ecdf = ecdf))
}

# Calculate ECDFs for good and bad wine residual sugar
ecdf.good.rs <- ecdf.func(good.wine$residual.sugar)
ecdf.bad.rs <- ecdf.func(bad.wine$residual.sugar)

# Plot ECDF for good wine
plot(ecdf.good.rs$sorted, ecdf.good.rs$ecdf, type = "s", main = "ECDFs of Residual Sugar", xlab = "Residual Sugar", ylab = "ECDF", col = "black", lty = 1)

# Add ECDF for bad wine
lines(ecdf.bad.rs$sorted, ecdf.bad.rs$ecdf, type = "s", col = "brown", lty = 2)

# Add legend
legend("bottomright", legend = c("Good Quality", "Bad Quality"), col = c("black", "brown"), lty = c(1, 2))
```

Here the definition of ECDF is important. I will simply use the one in the lecture in the rest of the report.
While the brown plot represents the ECDF of residual sugar for bad wine, the black one represents that of good wine. From the ECDF, we can identify where the most values occur. It's evident that there are more values of lower residual sugar for good wine than for bad wine. However, up to a certain point, we can say they distribute similarly. Afterwards, bad quality wines usually have more sugar.

3.  Consider now volatile.acidity for good and bad wines. Use boxplots, histograms, QQplots, summary statistics and empirical distribution functions to compare this variable for
good and bad wines. Comment on the results.

```{r volatile.acidity}
p <- ggplot(white.wine.quality, aes(good, volatile.acidity))
p + geom_boxplot()

hist(good.wine$volatile.acidity,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Volatile Acidity of Good Wine with Sturges' Rule",
 xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Volatile Acidity of Bad Wine with Sturges' Rule",
 xlab = "Volatile Acidity")


hist(good.wine$volatile.acidity,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Volatile Acidity of Good Wine with Scott's Rule",
     xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Volatile Acidity of Bad Wine with Scott's Rule",
     xlab = "Volatile Acidity")


hist(good.wine$volatile.acidity,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Volatile Acidity of Good Wine with Freedman-Diaconis Rule",
     xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Volatile Acidity of Bad Wine with Freedman-Diaconis Rule",
     xlab = "Volatile Acidity")


qqplot(bad.wine$volatile.acidity, good.wine$volatile.acidity, main = "QQ-plot: Good vs Bad Wines",
       xlab = "Volitile Acidity - Bad Wines", ylab = "Volitile Acidity - Good Wines")
abline(0, 1, col = "red")  # Add y = x line

white.wine.quality %>%
  group_by(good) %>%
  summarise(mean.va = mean(volatile.acidity),
                        median.va = median(volatile.acidity),
                        sd.va = sd(volatile.acidity),
                        IQR.va = IQR(volatile.acidity),
                        min.va = min(volatile.acidity),
                        max.va = max(volatile.acidity))


ecdf.good.va <- ecdf.func(good.wine$volatile.acidity)
ecdf.bad.va <- ecdf.func(bad.wine$volatile.acidity)

# Plot ECDF for good wine
plot(ecdf.good.va$sorted, ecdf.good.va$ecdf, type = "s", main = "ECDFs of Volatile Acidity", xlab = "Volatile Acidity", ylab = "ECDF", col = "black", lty = 1)

# Add ECDF for bad wine
lines(ecdf.bad.va$sorted, ecdf.bad.va$ecdf, type = "s", col = "brown", lty = 2)

# Add legend
legend("bottomright", legend = c("Good Quality", "Bad Quality"), col = c("black", "brown"), lty = c(1, 2))

```

Boxplots of both wine groups seem to have a very heavy and long right tail. The median of the bad wine group is slightly higher than that of the good wine group.

When we look at histograms, transitioning from Sturges' Rule to Scott's Rule, the number of bins increases for both good and bad wines. The reason why it did not differ in residual sugar might be that the variance in volatile acidity is much lower than the variance in residual sugar. Additionally, the IQR of volatile acidity is significantly lower than that of residual sugar in both wines. Eventually, the bin width of Freedman-Diaconis is very small in bad wines as well.

Looking at the QQ plot, we can see that the values of good wine are more concentrated or have less variance than those for bad wines because of the slope of the QQ plot. Additionally since values are below the y=x line, the good wine group is expected to have lower mean.

On the ECDF plot, we can see that there are more values of lower volatile acidity for good wine than for bad wine. This is also consistent with the fact that there were more bad wine values below the good wine values on the QQ plot.

## Examine the distribution of data

#### Dataset

Consider the dataset from the previous exercise and its variable pH.

#### Exercises

1- Plot a histogram of pH for all wines and add to the plot a normal density, estimating the parameters from the data. Produce same histograms with corresponding normal densities for good and bad wines separately. Do you observe any differences in the distributions?

```{r hist.FreedmanDiaconis.pH}

hist(good.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for All Wines with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(white.wine.quality$pH), sd = sd(white.wine.quality$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

hist(good.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for Good Wine with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(good.wine$pH), sd = sd(good.wine$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

hist(bad.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for Bad Wine with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(bad.wine$pH), sd = sd(bad.wine$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

```

Distributions seem more or less normal and similar. The pH of bad wine is slightly spikier than that of the pH of good wine. 

2- Generate QQ-plots of pH for good, bad and all wines to compare empirical quantiles of the samples to the theoretical quantiles of a normal distribution. Produce PP-plots for all three datasets. Comment on the differences between QQ-plots and PP-plots. Do you think all samples follow a normal distribution?

```{r qqplot.theoretical}
qqnorm(good.wine$pH, main="QQ-plot of pH for Good Wine")
qqline(good.wine$pH)

qqnorm(bad.wine$pH, main="QQ-plot of pH for Bad Wine")
qqline(bad.wine$pH)

qqnorm(white.wine.quality$pH, main="QQ-plot of pH for All Wines")
qqline(white.wine.quality$pH)

```

qqnorm() function plots the emprical distribution using ppoints() function in R. ppoints() uses (1:m - a)/(m + (1-a)-a) where m is either n, if length(n)==1, or length(n). And a by default a = if(n <= 10) 3/8 else 1/2).
Good wine and all wine datasets seem to have a lighter left tail and heavier right tail than the theoretical normal distribution. Bad wine seems to only have heavier right tail but left tail seems to be more or less similar to the normal distribution.

```{r ppplot.theoretical}

standardized.data <- (good.wine$pH - mean(good.wine$pH)) / sd(good.wine$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="PP-plot of pH for Good Wine")
abline(0,1)

standardized.data <- (bad.wine$pH - mean(bad.wine$pH)) / sd(bad.wine$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="PP-plot of pH for Bad Wine")
abline(0,1)

standardized.data <- (white.wine.quality$pH - mean(white.wine.quality$pH)) / sd(white.wine.quality$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="PP-plot of pH for All Wine")
abline(0,1)

```

The only difference between QQ and PP plot is that considering ECDFs on the same plot, whilst QQ-plot plots the corresponding values horizontally, PP-plot plots the vertical values. Therefore it makes more sense that tails are better expressed on qq-plot and mode is better expressed with pp-plot. Differences on tails are more clear on QQ-plots. But they more or less follow normal distributions.


3-) Plot the empirical distribution functions Fn for all three datasets. Add the pointwise confidence bands for α = 0.05 using the central limit theorem and Slutzky’s lemma (ensure that
the confidence bands are in [0; 1]).

```{r ecdf.pwbands.plot}
ecdf.pwbands.plot <- function(data, title){
  ecdf_data <- ecdf.func(data)
  
  z <- sqrt(ecdf_data$ecdf*(1-ecdf_data$ecdf) / length(ecdf_data$ecdf))
  
  plot(ecdf_data$sorted,ecdf_data$ecdf , col="lightgreen",type = "s", lwd=2, main = title)
  
  upper_bound <- ecdf_data$ecdf + 1.96 * z
  lower_bound <- ecdf_data$ecdf - 1.96 * z
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  lines(ecdf_data$sorted, upper_bound, lty = "dashed", col="salmon", lwd=2)
  lines(ecdf_data$sorted, lower_bound, lty = "dashed", col="salmon", lwd=2)
}

ecdf.pwbands.plot(good.wine$pH, "Good Wine pH ECDF and Pointwise Confidence Intervals")
ecdf.pwbands.plot(bad.wine$pH, "Bad Wine pH ECDF and Pointwise Confidence Intervals")
ecdf.pwbands.plot(white.wine.quality$pH, "All Wine pH ECDF and Pointwise Confidence Intervals")

```

4- Plot the empirical distribution functions Fn for all three datasets together with the uniform
confidence bands for α = 0.05. Compare to the bands obtained in (3).

```{r ecdf.ksbands}

ecdf.ksbands <- function(data) {
  ecdf_data <- ecdf.func(data)
  
  D <- 1.3581 / sqrt(length(data))
  upper_bound <- ecdf_data$ecdf + D
  lower_bound <- ecdf_data$ecdf - D
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  return(list(sorted = ecdf_data$sorted, ecdf = ecdf_data$ecdf, upper_bound = upper_bound, lower_bound = lower_bound))
}


ecdf.good.ph.ks <- ecdf.ksbands(good.wine$pH)

plot(ecdf.good.ph.ks$sorted, ecdf.good.ph.ks$ecdf,type = "s", col="lightgreen", lwd=2, main = "Good Wine pH ECDF and Uniform Confidence Intervals")
lines(ecdf.good.ph.ks$sorted, ecdf.good.ph.ks$upper_bound, col="salmon", lwd=2, lty = "dashed")
lines(ecdf.good.ph.ks$sorted, ecdf.good.ph.ks$lower_bound, col="salmon", lwd=2, lty = "dashed")
  
ecdf.bad.ph.ks <- ecdf.ksbands(bad.wine$pH)

plot(ecdf.bad.ph.ks$sorted, ecdf.bad.ph.ks$ecdf,type = "s", col="lightgreen", lwd=2, main = "Bad Wine pH ECDF and Uniform Confidence Intervals")
lines(ecdf.bad.ph.ks$sorted, ecdf.bad.ph.ks$upper_bound, col="salmon", lwd=2, lty = "dashed")
lines(ecdf.bad.ph.ks$sorted, ecdf.bad.ph.ks$lower_bound, col="salmon", lwd=2, lty = "dashed")
  
ecdf.all.ph.ks <- ecdf.ksbands(white.wine.quality$pH)

plot(ecdf.all.ph.ks$sorted, ecdf.all.ph.ks$ecdf, type = "s", col="lightgreen", lwd=2, main = "All Wine pH ECDF and Uniform Confidence Intervals")
lines(ecdf.all.ph.ks$sorted, ecdf.all.ph.ks$upper_bound, col="salmon", lwd=2, lty = "dashed")
lines(ecdf.all.ph.ks$sorted, ecdf.all.ph.ks$lower_bound, col="salmon", lwd=2, lty = "dashed")
  
```

Kolmogorov-Smirnov bandwidth for sample size over 35 with 0.05 alpha value is [1.36/sqrt(N)](https://real-statistics.com/statistics-tables/kolmogorov-smirnov-table/) = 0.02382667. The problem with pointwise confidence intervals is, that if we compute them for a lot of points, we accumulate errors. Uniform confidence bands are slightly wider. 

5- Plot the empirical distribution functions of pH for good and bad wines together with the uniform confidence bands in one plot. What can you conclude from this plot?

```{r ecdf.ksbands.ph}
# Create an empty plot with x-axis limits covering the range of both datasets

good_ph <- ecdf.ksbands(good.wine$pH)
bad_ph <- ecdf.ksbands(bad.wine$pH)

# Plot ECDFs for both good and bad wine pH in a single plot
plot(good_ph$sorted, good_ph$ecdf, type="s", col="darkgreen", lwd=2, 
     main="Wine pH ECDF and Uniform Confidence Intervals", xlim = range(c(good.wine$pH, bad.wine$pH)), ylim = c(0, 1), ylab = "ECDF", xlab = "pH")

lines(bad_ph$sorted, bad_ph$ecdf, col="darkred", lwd=2, type="s")

# Add confidence intervals for good wine
lines(good_ph$sorted, good_ph$upper_bound, col="blue", lwd=2, lty = "dashed")
lines(good_ph$sorted, good_ph$lower_bound, col="blue", lwd=2, lty = "dashed")

# Add confidence intervals for bad wine
lines(bad_ph$sorted, bad_ph$upper_bound, col="orange", lwd=2, lty = "dashed")
lines(bad_ph$sorted, bad_ph$lower_bound, col="orange", lwd=2, lty = "dashed")

# Add a legend
legend("bottomright", legend=c("Good Wine (ECDF)", "Bad Wine (ECDF)", "Good Wine (Confidence Intervals)", "Bad Wine (Confidence Intervals)"), 
       col=c("darkgreen", "darkred", "blue", "orange"), lwd=2, cex=0.8, lty=c("solid", "solid", "dashed", "dashed"))
```

On the ECDF we can see that up to a certain point, about to the pH value 3.1, good and bad wine pHs follow a similar distribution. Afterwards the gap increases as there are less bad wines with higher pH values. That is the pH of good wines is higher. Additionally, bad wines seem to have a larger confidence bands on tails compared to good wines.

## Maximum likelihood Estimation
Let X be Laplace distributed with parameters (µ, σ)
t ∈ R × (0, ∞), i.e., X has density

\[ f(x | \mu, σ) = \frac{1}{2σ} \exp\left(-\frac{|x - \mu|}{σ}\right) \]

#### Exercises
1- Consider a sample of independent observations (X1, . . . , Xn). Show that the maximum likelihood estimator for µ is the median of the sample. Is it unique?


Check out this informative [blog](https://math.stackexchange.com/questions/240496/finding-the-maximum-likelihood-estimator
).

Consider the likelihood function for N data samples:
\[L(\mu,\sigma;x) = \prod_{t=1}^{N} \frac{1}{2\sigma} e^{-\frac{|x_t-\mu|}{\sigma}} = (2\sigma)^{-N} e^{-\frac{1}{\sigma}\sum_{t=1}^{N}|x_t-\mu|}\]

Take the log likelihood function:

\[l(\mu,\sigma;x) = -N \ln(2\sigma) - \frac{{1}}{\sigma} \sum_{t=1}^{N}|x_t-\mu|\]


Take the derivative with respect to the parameter $\mu$:
\[ 
\frac{\partial l}{\partial \mu} = -\frac{1}{\sigma} \sum_{t=1}^{N} \frac{\partial |x_t-\mu|}{\partial \mu}
\]

which is equal to:
\[
\frac{1}{\sigma} \sum_{t=1}^{N} \text{sgn}(x_t-\mu) = 0 \quad (1)
\]

The sign function, usually denoted as $\text{sgn}(x)$, returns:
\[
\text{sgn}(x) = 
\begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x = 0 \\
-1 & \text{if } x < 0 
\end{cases}
\]

The median is a solution for both cases when N is an odd or even number. However, it is not unique for the even case. If N is even, {\mu} can take values strictly between $> x_{\left(\frac{n}{2}\right)}$ and $< x_{\left(\frac{n}{2}+1\right)}$. If it is odd it is unique and is equal to the median.

2- Generate n = 20 independent realisations of X with µ = 1 and σ = 1. Determine the maximum likelihood estimator of µ based on this sample using R function quantile. Function quantile has 9 types of sample quantiles. Experiment with the different types of quantiles that are most suitable for the data (justify). Are there any differences? Increase now the sample to n = 1000 and compare different median estimators. Comment on the result.

```{r laplace.quantile}
# install.packages('jmuOutlier')
library(jmuOutlier)
set.seed(123)

# Set parameters
mu <- 1
sigma <- 1

# Generate 20 independent realizations
n <- 20
laplace.data.20 <- rlaplace(n, mu, sigma)

for (i in 1:9) {
  cat("20 Samples Type ", i, " Median: ", quantile(laplace.data.20, 0.5, type = i), "\n")
}


n <- 1000
laplace.data.1000 <- rlaplace(n, mu, sigma)

for (i in 1:9) {
  cat("1000 Samples Type ", i, " Median: ", quantile(laplace.data.1000, 0.5, type = i), "\n")
}

```

To generate 20 independent realizations of a Laplace distribution with parameters µ = 1 and σ = 1 in R, you can use the rlaplace() function from the jmuOutlier package. They result in the same pattern. Type 1,3 and 4 are the same and the rest are another value. The reason for this is that for types 5 through 9, a different version of linear interpolation is applied. Type 4 is the linear interpolation of the empirical cdf. Type 2 averages at discontinuities and since we have even sample sizes and we are interested in the median, they all result in the same value. Therefore, type 1,3 and 4 rounds down and the rest takes the average of the 10th and 11th or the 500th and the 501st. In our case, type 2,5,6,7,8 and 9 can be used for an unbiased estimator.
When comparing the results for n = 20 and n = 1000, you'll notice that the medians are very close. This is expected because as the sample size increases, the sample median tends to converge to the population median, resulting in more consistent estimates regardless of the quantile type used.

3- Write your own function that calculates the maximum likelihood estimator for a Laplace sample numerically using R function optimise. Describe how R function optimise finds
the maximum. Can you employ a Newton-Raphson algorithm for this problem? Generate n = 20 and n = 1000 independent realisations of X with µ = 1 and σ = 1. Calculate the maximum likelihood estimators based on both samples with your function and using quantile. Compare both estimators, comment on the results.

```{r laplace}

laplace_data <- laplace.data.1000
  laplace.log.likelihood <- function(mu) {
    (-length(laplace_data)*log(2 *sigma) -(1/sigma) * sum(abs(laplace_data - mu)))
  }

optimise(laplace.log.likelihood,maximum = T,
         lower = 0, upper = 10)

laplace_data <- laplace.data.20
optimise(laplace.log.likelihood,maximum = T,
         lower = 0, upper = 10)



laplace_data <- laplace.data.1000

# Newton-Raphson
niter <- 100
# xk Initial values
laplace.log.likelihood.newton <- function(xk){
  h <- 0.01
  for(k in 1:niter){
    Q <- (h/2) * ((laplace.log.likelihood(xk + h) - laplace.log.likelihood(xk - h))/
                    (laplace.log.likelihood(xk + h) - 2*laplace.log.likelihood(xk) + laplace.log.likelihood(xk - h)))
    xnew <- xk - Q
    print(xnew)
    xk <- xnew
  }
}

laplace.log.likelihood.newton(0.21)


laplace_data <- laplace.data.20
laplace.log.likelihood.newton(0.21)

```

According to the [documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optimize), optimise() method uses somewhat a combination of golden section search and successive parabolic interpolation. The algorithm starts with an initial interval and evaluates the function at two points within this interval. The points are chosen according to the golden section ratio $φ = (1 + √5) / 2$, which ensures that the interval contracts efficiently.  After the initial evaluations, the algorithm performs successive parabolic interpolations to refine the location of the maximum. This technique utilizes information from previous function evaluations to approximate the maximum more accurately. We get very close optimum results with the corresponding data.
Check out this informative video on YouTube: [YouTube Video](https://www.youtube.com/watch?v=8z4I348eayg).


Newton-Raphson method:
\[x_{k+1} = x_k - Q_k\]
\[Q_k = \frac{f'(x_k)}{f''(x_k)}\]

The method tends to converge if the following condition holds true:
\[|f(x)f''(x)| < |f'(x)|^2\]

Using the central difference approximation for the derivative:
\[Q_k = \frac{h}{2} \left[ \frac{f(x + h) - f(x - h)}{f(x + h) - 2f(x) + f(x - h)} \right]\]

For a small sample size like n=20, even though the log-likelihood function tries to smooth out, it can be highly irregular due to the limited data. This irregularity can cause the Newton-Raphson method (or even optimise) to perform poorly. Both performs well with the sample size 1000 however, newton-raphson does not converge.

4-) Let us now study the distribution of the maximum likelihood estimator. For this, calculate M = 5000 maximum likelihood estimators of µ based on the sample of n = 20 random variables generated from the Laplace distribution with µ = 1 and σ = 1. Repeat the same for the sample size n = 1000. Use histograms and QQ-plots to check if both Monte Carlo samples follow a normal distribution. Compare variances of both distributions, comment on the results.

```{r}
# Function to find MLE for mu using optimise
find_mle_laplace <- function(x) {
  result <- optimise(function(mu) laplace.log.likelihood(mu), maximum = TRUE, lower = -10, upper = 10)
  return(result$maximum)
}

# Newton-Raphson method
laplace.log.likelihood.newton <- function(xk){
  h <- 0.01
  tol = 1e-6
  for(k in 1:niter){
    Q <- (h/2) * ((laplace.log.likelihood(xk + h) - laplace.log.likelihood(xk - h))/
                    (laplace.log.likelihood(xk + h) - 2*laplace.log.likelihood(xk) + laplace.log.likelihood(xk - h)))
    xnew <- xk - Q
    
    if(is.na(xnew)){
      return("No Convergence")
    }
    
    if (abs(xnew - xk) < tol) {
      return(xnew)  # Convergence check
    }
    xk <- xnew
  }
  return(xk)
}

# Parameters
mu <- 1
sigma <- 1
n <- 20
M <- 5000

# Storage for results
results_optimise <- numeric(M)
results_quantile <- numeric(M)
results_newton <- numeric(M)

for (i in 1:M) {
  laplace_data <- rlaplace(n, mu, sigma)
  
  # Quantile (median)
  quantile_result <- quantile(laplace_data, 0.5, type = 5)
  results_quantile[i] <- quantile_result
  
  # MLE using optimise
  optimise_result <- find_mle_laplace(laplace_data)
  results_optimise[i] <- optimise_result
  
  # MLE using Newton-Raphson
  newton_result <- laplace.log.likelihood.newton(0.21)
  results_newton[i] <- newton_result
}

# Display results
cat("First 5 results using optimise:\n", head(results_optimise, 5), "\n")
cat("First 5 results using quantile:\n", head(results_quantile, 5), "\n")

# Plot histograms and QQ-plots
par(mfrow = c(3, 2))
hist(results_optimise, main = "Histogram of MLEs (optimise) for n = 20", xlab = "MLE of mu", breaks = 50)
qqnorm(results_optimise, main = "QQ-plot of MLEs (optimise) for n = 20")
qqline(results_optimise)

hist(results_quantile, main = "Histogram of MLEs (quantile) for n = 20", xlab = "MLE of mu", breaks = 50)
qqnorm(results_quantile, main = "QQ-plot of MLEs (quantile) for n = 20")
qqline(results_quantile)

# Variance comparison
variance_optimise <- var(results_optimise)
variance_quantile <- var(results_quantile)

cat("Variance of MLEs using optimise for n = 20:", variance_optimise, "\n")
cat("Variance of MLEs using quantile for n = 20:", variance_quantile, "\n")

# n 1000
n = 1000

for (i in 1:M) {
  laplace_data <- rlaplace(n, mu, sigma)
  
  # Quantile (median)
  quantile_result <- quantile(laplace_data, 0.5, type = 5)
  results_quantile[i] <- quantile_result
  
  # MLE using optimise
  optimise_result <- find_mle_laplace(laplace_data)
  results_optimise[i] <- optimise_result
  
  # MLE using Newton-Raphson
  newton_result <- laplace.log.likelihood.newton(0.3)
  results_newton[i] <- newton_result

}
results_newton <- subset(results_newton, as.numeric(results_newton) >= -10 &
                          as.numeric(results_newton) <= 10)


results_newton <- as.numeric(results_newton)


# Display results
cat("First 5 results using optimise:\n", head(results_optimise, 5), "\n")
cat("First 5 results using quantile:\n", head(results_quantile, 5), "\n")
cat("First 5 results using Newton-Raphson:\n", head(results_newton, 5), "\n")

# Plot histograms and QQ-plots
par(mfrow = c(3, 2))
hist(results_optimise, main = "Histogram of MLEs (optimise) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_optimise, main = "QQ-plot of MLEs (optimise) for n = 1000")
qqline(results_optimise)

hist(results_quantile, main = "Histogram of MLEs (quantile) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_quantile, main = "QQ-plot of MLEs (quantile) for n = 1000")
qqline(results_quantile)

hist(results_newton, main = "Histogram of MLEs (Newton-Raphson) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_newton, main = "QQ-plot of MLEs (Newton-Raphson) for n = 1000")
qqline(results_newton)

# Variance comparison
variance_optimise <- var(results_optimise)
variance_quantile <- var(results_quantile)
variance_newton <- var(results_newton)

cat("Variance of MLEs using optimise for n = 1000:", variance_optimise, "\n")
cat("Variance of MLEs using quantile for n = 1000:", variance_quantile, "\n")
cat("Variance of MLEs using Newton-Raphson for n = 1000:", variance_newton, "\n")
```

## Linear Regression

#### Dataset
Go to Kaggle.com and download the data on house prices. This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. There are altogether 19 variables, but we will consider in the analysis only the following ones:

-   price: Price
-   bedrooms: Number of bedrooms
-   bathrooms: Number of bathrooms per bedroom
-   sqft_living: Square footage of the home
-   floors: Total floors in house
-   view: Has been viewed (1 for viewed; 0 for not viewed)
-   condition: How good is the condition (from 1 to 5)
-   grade: Grade given to the housing unit based on King County grading system (from 1 to 13)
-   yr_built: Year the house was built

```{r house.price}
kc_house_data <- read.csv("archive/kc_house_data.csv")

kc_house_data <- kc_house_data %>%
  select(price, bedrooms, bathrooms,
         sqft_living,
         floors,
         view,
         condition,
         grade,
         yr_built)

head(kc_house_data)
```

#### Exercises

1-) Estimate a linear model with the response variable price and all remaining variables as covariates. Are all variables significant? How large is R2 and how can this be interpreted?
Perform the residual analysis to validate the model. Are there any departures from the linear regression model assumptions?

```{r lm}
model <- lm(price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = kc_house_data)

summary(model)
```

The estimated effect of each predictor on the response variable (price). For instance, each additional bedroom decreases the price by approximately 40,650 dollars, while each additional bathroom increases it by about 47,690 dollars. Pr(>|t|): The p-value associated with the t-statistic, testing the null hypothesis that the coefficient is equal to zero (no effect). A lower p-value (typically < 0.05) indicates that the predictor is statistically significant. All variables here are highly significant (p-values < 2e-16), suggesting strong evidence against the null hypothesis. 
R-squared, the proportion of variance in the response variable explained by the predictors. Here, 63.59% of the variance in price is explained by the model. Adjusted R-squared is adjusted for the number of predictors in the model, providing a more accurate measure when multiple predictors are used.
The summary indicates a strong model fit, with all predictors being statistically significant and the model explaining a substantial portion of the variance in house prices. However, the relatively high residual standard error suggests that while the model captures a significant portion of the variability, there is still considerable unexplained variation in house prices.

```{r lm.plot}
par(mfrow = c(2, 2))
plot(model)
```
On the Residuals vs Fitted plot, homoscedasticity of ordinary least squares estimation is clearly violated. Variance in residuals increases as fitted values increase. We can additionally support the violation of homoscedasticity by looking at the crookedness of Scale-Location plot. Q-Q plot of residuals is violated too. There are strong lower and upper tails. On the Residuals vs Leverage plot, even though there are points that are close to boundaries, most of them are within the range. So we can say that there are not many influential or outlier cases in linear regression analysis.

2-) Produce a histogram and a QQ-plot of the response variable price, as well as of its log transform log(price). Compare both distributions to the normal one. Fit now a linear model with the response variable log(price). Compare the estimated model with the one from (a) in terms of R2, significance and effect of covariates and model fit (via residual analysis). Which model is more adequate?

```{r plot.log}

kc_house_data$log_price <- log(kc_house_data$price)

par(mfrow = c(2, 2))
# Histogram of price
hist(kc_house_data$price, main = "Histogram of Price", xlab = "Price", breaks = 30)

# QQ-plot of price
qqnorm(kc_house_data$price, main = "QQ-Plot of Price")
qqline(kc_house_data$price)

# Histogram of log(price)
hist(kc_house_data$log_price, main = "Histogram of Log(Price)", xlab = "Log(Price)", breaks = 30)

# QQ-plot of log(price)
qqnorm(kc_house_data$log_price, main = "QQ-Plot of Log(Price)")
qqline(kc_house_data$log_price)

```

Both the Histogram and QQ-Plot of price shows a very high right tail. Log(price) still has a slightly heavy tail but it is considerably very low.

```{r lm.log}
# Fit the linear model with log(price) as response
model_log_price <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = kc_house_data)

# Summary of the model
summary(model_log_price)
```

The R-squared values are slightly higher in the log model, indicating that this model explains a bit more variance in the response variable compared to the original model. In both models, all predictors are statistically significant with p-values < 2e-16. The t-values for the predictors are generally higher in the log model, suggesting stronger relationships between predictors and the log-transformed response variable. In the log transformed models, the interpretation of coefficients is slightly different. Following the bathroom independent variable, each unit of increase now means, the dependent variable is multiplied by $e^(169.3)$.

```{r lm.log.plot}
par(mfrow = c(2, 2))
plot(model_log_price)
```

Residuals vs Fitted and Scale-Location plots shows that homoscedasticity problem is highly resolved. Q-Q Plot also follows a straight line. And we cannot even see the most of dashed lines in Residuals vs Leverage. The point 12778 seems pretty close to the one below tho.

In summary, the model with log(price) as the response variable appears to be more appropriate as it:

-   Has a higher R-squared value, indicating a better fit.
-   Shows stronger relationships between predictors and the response variable.
-   Demonstrates improved residual distribution, suggesting fewer outliers and a more consistent error structure.

3-) In the model from (b) interpret the effect of each covariate on the response. Plot each covariate against log(price). Is the assumption of the linear dependence between covariates and
response plausible for all covariates? Add to the model from (b) squared terms for yr_built
and sqft_living. Are these terms significant? Does adding these two terms improve the
model fit in terms of R2?

```{r covariates, warning=FALSE, message=FALSE}
covariates <- c("bedrooms", "bathrooms", "sqft_living", "floors", "view", "condition", "grade", "yr_built")

# Create plots
for (var in covariates) {
  p <- ggplot(kc_house_data, aes_string(x = var, y = "log_price")) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", col = "blue") +
    labs(title = paste("Log(price) vs", var), x = var, y = "Log(price)")
  print(p)
}
```

Variables bathrooms and grade can likely have a linear relationship. But the rest of variables seem to either have a different relationship such as sqft_living or they don't explain any variance in the predictor variables such as yr_built.

```{r squared.lm}
# Fit the new model with squared terms
model_log_price_sq <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built + I(yr_built^2) + I(sqft_living^2), data = kc_house_data)

summary(model_log_price_sq)
```

The terms are significant. And the R2 value is slightly improved from 0.6426 to 0.6492. Adjusted R-Squared is increased as well.

4-) Now we would like to compare how well models from (b) and (c) make prediction. For this divide the dataset into a training and a test set. Sample randomly 10 806 rows to include into the training set and the rest will be the test set. To ensure comparability of the results set.seed(1122) before sampling. Fit both models on the training set and make prediction on the test set. Calculate the mean squared difference between predicted values and values of log(price) from the test set for each model. Which prediction error is smaller? Try to extend the model to improve the prediction: my best model gives prediction error of 0.09557445.

```{r mse.extention}
set.seed(1122)
training_indices <- sample(seq_len(nrow(kc_house_data)), size = 10806)
training_set <- kc_house_data[training_indices, ]
test_set <- kc_house_data[-training_indices, ]

model_log_price_train <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = training_set)

model_log_price_sq_train <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built + I(yr_built^2) + I(sqft_living^2), data = training_set)

pred_log_price <- predict(model_log_price_train, newdata = test_set)

pred_log_price_sq <- predict(model_log_price_sq_train, newdata = test_set)

mse_log_price <- mean((test_set$log_price - pred_log_price)^2)
mse_log_price

mse_log_price_sq <- mean((test_set$log_price - pred_log_price_sq)^2)
mse_log_price_sq

summary(model_log_price_sq_train)

# Extention

model_log_price_new <- lm(log_price ~ bedrooms +  bedrooms : bathrooms  +  bathrooms : grade + sqft_living +sqft_living:floors + grade + floors  + condition + yr_built + I(yr_built^2) + I(sqft_living^2) +view + view:bathrooms  + I(floors^2) +I(grade^2), data = training_set)

pred_log_price_new <- predict(model_log_price_new, newdata = test_set)

mse_log_price_new <- mean((test_set$log_price - pred_log_price_new)^2)

print(paste0("Prediction error: ", mse_log_price_new))
summary(model_log_price_new)

```

Although the model model_log_price_new  or pred_log_price_sq performs slightly better, it usually does not mean the model generalizes better. I particularly had to try many different interaction effects or polynomial fits on this split. With different splits, different interactions or effects can give different results.


## Penalized Regression

#### Dataset

Consider the dataset hitters.dat on baseball players that has been analysed in the book An introduction to statistical learning by G. James, D. Witten, T. Hastie and R. Tibshirani (2013, Springer). It contains the following 20 variables:

-   AtBat: Number of times at bat in 1986
-   Hits: Number of hits in 1986
-   HmRun: Number of home runs in 1986
-   Runs: Number of runs in 1986
-   RBI: Number of runs batted in 1986
-   Walks: Number of walks in 1986
-   Years: Number of years in the major leagues
-   CAtBat: Number of times at bat during his career
-   CHits: Number of hits during his career
-   CHmRun: Number of home runs during his career
-   CRuns: Number of runs during his career
-   CRBI: Number of runs batted in during his career
-   CWalks: Number of walks during his career
-   League: A factor with levels A and N indicating player’s league at the end of 1986
-   Division: A factor with levels E and W indicating player’s division at the end of 1986
-   PutOuts: Number of put outs in 1986
-   Assists: Number of assists in 1986
-   Errors: Number of errors in 1986
-   Salary: 1987 annual salary on opening day in thousands of dollars
-   NewLeague: A factor with levels A and N indicating player’s league at the beginning of 1987

We would like to explain Salary of a player using the other variables. The function glmnet in the package glmnet is used to fit both ridge and lasso regression. Supply α = 0 for ridge and α = 1 for
lasso when calling glmnet. Make sure you understand the interpretation of λ in both cases (note that it is not the same for lasso and ridge!). Read the documentation ?glmnet to understand what
is the exact form of the objective function that is being minimised.


1-) Load the dataset into R and create a new dataset containing only those players for which all
data is available.

```{r hitters.df}
Hitters <- Hitters[complete.cases(Hitters),]

```

2-) Find the condition number (ratio between largest and smallest eigenvalue) of $X^T X$, where y is the salary and X represents all the other variables. What can you say about the condition number? Does it help if you standardise the design matrix, such that its columns (without the intercept) have mean zero and variance one?

```{r condition.number.hitters}

x = model.matrix(Salary~., Hitters)[,-1]
condition.number <- function (x){
  z <- t(x) %*% x
  print(max(svd(z)$d)/min(svd(z)$d))
  print(kappa(z, exact = T))
  }

condition.number(x)
condition.number(scale(x))

```

The condition number is a good indicator of how close is a matrix to be singular. The larger the condition number the closer we are to singularity. A matrix with large condition number is nearly singular, whereas a matrix with a condition number close to 1 is far from being singular.
Let $\boldsymbol{x}$ be the solution of {\bf A} \boldsymbol{x} = \boldsymbol{b}
 be the solution of the perturbed problem ${\bf A} \hat{\boldsymbol{x}} = \boldsymbol{b} + \Delta \boldsymbol{b}$ . Let $\Delta \boldsymbol{x} = \hat{\boldsymbol{x}} - \boldsymbol{x}$ be the absolute error in output. Then we have ${\bf A} \boldsymbol{x} + {\bf A} \Delta \boldsymbol{x} = \boldsymbol{b} + \Delta \boldsymbol{b}$, so ${\bf A} \Delta \boldsymbol{x} = \Delta \boldsymbol{b}.$ Now we want to see how the relative error in output $\left(\frac{\|\Delta \boldsymbol{x}\|}{\|\boldsymbol{x}\|}\right)$ is related to the relative error in input $\left(\frac{\|\Delta \boldsymbol{b}\|}{\|\boldsymbol{b}\|}\right)$

\[
\begin{align}
\frac{\|\Delta \boldsymbol{x}\| / \|\boldsymbol{x}\|}{\|\Delta \boldsymbol{b}\| / \|\boldsymbol{b}\|} &= \frac{\|\Delta \boldsymbol{x}\| \|\boldsymbol{b}\|}{\|\boldsymbol{x}\| \|\Delta \boldsymbol{b}\|}\\
&= \frac{\|{\bf A}^{-1} \Delta \boldsymbol{b}\| \|{\bf A} \boldsymbol{x}\|}{\|\boldsymbol{x}\| \|\Delta \boldsymbol{b}\|}\\
&\le \frac{\|{\bf A}^{-1}\| \|\Delta \boldsymbol{b}\| \|{\bf A}\| \|\boldsymbol{x}\|}{\|\boldsymbol{x}\| \|\Delta \boldsymbol{b}\|} \\
&= \|{\bf A}^{-1}\| \|{\bf A}\|\\ &= \text{cond}({\bf A})
\end{align}\]

where we used $\|{\bf A}\boldsymbol{x}\| \le \|{\bf A}\| \|\boldsymbol{x}\|, \forall \boldsymbol{x}$.
Then
\[
\frac{\|\Delta \boldsymbol{x}\|}{\|\boldsymbol{x}\|} \le \text{cond}({\bf A})\frac{\|\Delta \boldsymbol{b}\|}{\|\boldsymbol{b}\|}  \qquad (1)
\]

Therefore, if we know the relative error in input, then we can use the condition number of the system to obtain an upper bound for the relative error of our computed solution (output). For more information, check out the [link](https://courses.engr.illinois.edu/cs357/sp2024/notes/ref-10-condition.html).


When the matrix is standardized, condition number decreases drastically. In this [answer](https://stats.stackexchange.com/questions/243000/cause-of-a-high-condition-number-in-a-python-statsmodels-regression#comment462188_243000) the reason for a high condition number is clearly explained. This is because they're fitting a line to the points and then projecting the line all the way back to the origin (x=0) to find the y-intercept. That y-intercept will be very sensitive to small movements in the data points. The condition number takes into account high sensitivity in either fitted parameter to the input data, hence the high condition number when all of the data are far to one side of x=0.

 
3-) Fit a standard linear model (no regularisation) and a ridge regression with λ = 70. Compare the size of the coefficients in the two models. What do you observe?

```{r lm.hitters}
y = Hitters %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

# Fit a standard linear model
lm_model <- lm(Salary ~ ., data = Hitters)

# Get the coefficients of the linear model
lm_coefficients <- coef(lm_model)
print("Linear Model Coefficients:")
print(lm_coefficients)


# Fit a ridge regression model with lambda = 70
ridge_model <- glmnet(x, y, alpha = 0, lambda = 70)

# Get the coefficients of the ridge regression model
ridge_coefficients <- coef(ridge_model)
print("Ridge Regression Coefficients with lambda = 70:")
print(ridge_coefficients)



# Standard Linear Model Coefficients
lm_coefficients <- coef(lm_model)
lm_coefficients <- as.data.frame(lm_coefficients)
colnames(lm_coefficients) <- "LinearModel"

# Ridge Regression Coefficients with lambda = 70
ridge_coefficients <- coef(ridge_model)
ridge_coefficients <- as.data.frame(as.matrix(ridge_coefficients))
colnames(ridge_coefficients) <- "RidgeRegression"

# Combine the data frames, making sure row names are kept as a column
comparison_df <- data.frame(
  Variable = rownames(lm_coefficients),
  LinearModel = lm_coefficients$LinearModel,
  RidgeRegression = ridge_coefficients$RidgeRegression
)

comparison_df_tidy <- pivot_longer(
  comparison_df,
  cols = c("LinearModel", "RidgeRegression"),
  names_to = "Model",
  values_to = "Coefficient"
)


ggplot(comparison_df_tidy, aes(x = Variable, y = Coefficient, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Comparison of Coefficients: Linear Model vs. Ridge Regression",
       x = "Variable",
       y = "Coefficient Value")

```

glm() by default standardizes the data. The importance of standardization in regularized models are thoroughly examined in the exercise before. Here in Ridge Regression, the coefficients are generally smaller, indicating that the regularization term λ is effectively shrinking them. This often results in a model that generalizes better to new data by mitigating multicollinearity and reducing overfitting.

4-) The value 70 for λ is arbitrary and we would like to find a data-driven way to choose it. The criterion to compare is the mean squared prediction error as in exercise 4 (d) on linear regression. Split the data randomly into a training and a test set with set.seed(1122).

```{r empty}

set.seed(1122)

train = Hitters %>%
  sample_frac(0.8)

test = Hitters %>%
  setdiff(train)

x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]

y_train = train %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

y_test = test %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam

plot(cv.out)

# What is the test MSE associated with this value of  λ?
ridge_model.best.l <- glmnet(x_train, y_train, alpha=0, lambda = bestlam)
ridge_pred = predict(ridge_model.best.l, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((ridge_pred - y_test)^2)
```

I think the best way to perform this task would be to use the built-in cross-validation function, cv.glmnet(). By default, the function performs 10-fold cross-validation.

5-) Write a function that takes λ as argument, fits a ridge regression on the training sets and calculates the mean squared prediction error on the test set. Run this function on a logarithmic grid (e.g., 10^seq(from = 10, to = -2, length = 100)). Plot the results against log(λ) and graphically find the value λopt that minimises the mean squared prediction error.

```{r}
ridge_mse <- function(lambda, x_train, y_train, x_test, y_test) {
  ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = lambda)
  y_pred <- predict(ridge_mod, s = lambda, newx = x_test)
  mse <- mean((y_test - y_pred)^2)
  return(mse)
}

# Grid of lambda values
lambda_grid <- 10^seq(10, -2, length = 100)

# Calculate MSE for each lambda
mse_values <- sapply(lambda_grid, ridge_mse, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test)

# Find the optimal lambda that minimizes MSE
optimal_lambda <- lambda_grid[which.min(mse_values)]
optimal_mse <- min(mse_values)

# Plot the results
plot_data <- data.frame(
  log_lambda = log(lambda_grid),
  mse = mse_values
)

ggplot(plot_data, aes(x = log_lambda, y = mse)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = log(optimal_lambda), linetype = "dashed", color = "red") +
  annotate("text", x = log(optimal_lambda), y = optimal_mse, label = paste0("lambda: ", round(optimal_lambda, 2), "\nMSE: ", round(optimal_mse, 2)), hjust = -0.1, vjust = -1.5, color = "red") +
  labs(title = "MSE vs. log(lambda)",
       x = "log(lambda)",
       y = "Mean Squared Error") +
  theme_minimal()

# Print the optimal lambda value
cat("The optimal lambda value is:", optimal_lambda, "\n")

```

6-)  Fit a ridge regression with λopt on all the data, and interpret some of the coefficients. Which are the most important variables? Are there coefficients that equal zero exactly?

```{r}

# Fit a ridge regression with optimal lambda on all the data
ridge_model_optimal <- glmnet(x, y, alpha = 0, lambda = optimal_lambda)
optimal_ridge_coefficients <- as.vector(coef(ridge_model_optimal))
names(optimal_ridge_coefficients) <- rownames(coef(ridge_model_optimal))

print("Ridge Regression Coefficients with optimal lambda:")

important_variables <- sort(abs(optimal_ridge_coefficients), decreasing = TRUE)
important_variables


```
Intercept and DivisionW are pretty important in ridge regression. There are not any coefficients that are set to 0.


7-) Repeat parts (d), (e) and (f) for lasso instead of ridge. Are there now coefficients that are equal to zero?


```{r}

# Fit lasso regression model with cross-validation
cv.out_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
bestlam_lasso <- cv.out_lasso$lambda.min

# Plot cross-validation results
plot(cv.out_lasso)

# Calculate the test MSE associated with the best lambda for lasso
lasso_model_best <- glmnet(x_train, y_train, alpha = 1, lambda = bestlam_lasso)
lasso_pred <- predict(lasso_model_best, s = bestlam_lasso, newx = x_test)
test_mse_lasso <- mean((lasso_pred - y_test)^2)
cat("The test MSE associated with the best CV lambda for lasso is:", test_mse_lasso, "\n")

# Define the function to calculate MSE
lasso_mse <- function(lambda, x_train, y_train, x_test, y_test) {
  lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = lambda)
  y_pred <- predict(lasso_mod, s = lambda, newx = x_test)
  mse <- mean((y_test - y_pred)^2)
  return(mse)
}

# Grid of lambda values
lambda_grid_lasso <- 10^seq(10, -2, length = 100)

# Calculate MSE for each lambda
mse_values_lasso <- sapply(lambda_grid_lasso, lasso_mse, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test)

# Find the optimal lambda that minimizes MSE
optimal_lambda_lasso <- lambda_grid_lasso[which.min(mse_values_lasso)]
optimal_mse_lasso <- min(mse_values_lasso)

# Plot the results
plot_data_lasso <- data.frame(
  log_lambda = log(lambda_grid_lasso),
  mse = mse_values_lasso
)

ggplot(plot_data_lasso, aes(x = log_lambda, y = mse)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = log(optimal_lambda_lasso), linetype = "dashed", color = "red") +
  annotate("text", x = log(optimal_lambda_lasso), y = optimal_mse_lasso, label = paste0("lambda: ", round(optimal_lambda_lasso, 2), "\nMSE: ", round(optimal_mse_lasso, 2)), hjust = -0.1, vjust = -1.5, color = "red") +
  labs(title = "MSE vs. log(lambda) for Lasso",
       x = "log(lambda)",
       y = "Mean Squared Error") +
  theme_minimal()

cat("The optimal lambda value for lasso is:", optimal_lambda_lasso, "\n")


# Fit a lasso regression with optimal lambda on all the data
lasso_model_optimal <- glmnet(x, y, alpha = 1, lambda = optimal_lambda_lasso)
optimal_lasso_coefficients <- as.vector(coef(lasso_model_optimal))
names(optimal_lasso_coefficients) <- rownames(coef(lasso_model_optimal))

important_lasso_variables <- sort(abs(optimal_lasso_coefficients), decreasing = TRUE)
important_lasso_variables


```

The difference between ridge and lasso regression is that Lasso tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero. Therefore, Lasso puts more weights on individual coefficients. Intercept, Hits, Walks, CRBI and CRuns are all non zero in Lasso regression. 


References

https://math.stackexchange.com/questions/240496/finding-the-maximum-likelihood-estimator

https://www.youtube.com/watch?v=8z4I348eayg