---
title: "Exercise 1"
output: html_document
date: "2024-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sample Statistics

#### Dataset

Go to UCI Machine learning repository and download the data on the white wine quality. This page contains also the background information on the data. In our analysis we will only consider the following variables:

-   volatile.acidity: Volatile acidity

-   residual.sugar: Residual sugar

-   pH: pH level

-   quality: Wine quality in a score between 0 and 10

#### Exercises

1. Read the data into R. Add to the data frame a new binary variable good which is 1 if quality \> 5 and 0 otherwise. We would like to compare volatile.acidity and residual.sugar for good and bad wines.

```{r a, warning=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(ggplot2)

white.wine.quality <- read.csv("winequality-white.csv", sep = ";")
white.wine.quality <- white.wine.quality %>% select(volatile.acidity, residual.sugar, 
                              pH, quality)

white.wine.quality <- white.wine.quality %>% 
  mutate(good = quality > 5)

head(white.wine.quality)
```

2. First consider variable residual.sugar.

+ Plot histograms of residual.sugar for good and bad wines using different methods available in R to choose the bin width. Comment on the shape of both histograms and differences in distribution, if any.

```{r hist.Sturges}

good.wine <- white.wine.quality[white.wine.quality$good == 1,]
bad.wine <- white.wine.quality[white.wine.quality$good == 0,]

hist(good.wine$residual.sugar,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Residual Sugar of Good Wine with Sturges' Rule",
 xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Residual Sugar of Bad Wine with Sturges' Rule",
 xlab = "Residual Sugar")
```

There are 1640 bad wine and 3258 good wine rows. The distribution of good wine seems to have a long tail on the right. The number of bins for good wine is 14 and it is 12 for bad wine when Sturges formula is used.

```{r hist.ScottsChoice}
hist(good.wine$residual.sugar,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Residual Sugar of Good Wine with Scott's Rule",
     xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Residual Sugar of Bad Wine with Scott's Rule",
     xlab = "Residual Sugar")
```

When Scott's choice is used, the number of breaks remains the same for the bad wine, but it increases significantly for the good wine. This allows us to capture more information, but it also introduces more noise.

```{r hist.FreedmanDiaconis}
hist(good.wine$residual.sugar,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Residual Sugar of Good Wine with Freedman-Diaconis Rule",
     xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Residual Sugar of Bad Wine with Freedman-Diaconis Rule",
     xlab = "Residual Sugar")
```

When Freedman-Diaconis' choice is used, the number of breaks is the same as with Scott's choice. Both Freedman-Diaconis' and Scott's choices take variability into account. However, Freedman-Diaconis' choice utilizes the interquartile range, which can make it more robust to outliers.

+   Calculate the summary statistics for both wine groups, that is: mean, median, standard deviation, interquartile range, minimum and maximum of both samples. Display the results in a table and comment on the differences between both groups, if any.


```{r summary}
white.wine.quality %>%
  group_by(good) %>%
  summarise(mean.rs = mean(residual.sugar),
                        median.rs = median(residual.sugar),
                        sd.rs = sd(residual.sugar),
                        IQR.rs = IQR(residual.sugar),
                        min.rs = min(residual.sugar),
                        max.rs = max(residual.sugar))

```
Considering that both of these variables from the good and bad groups are bounded by 0, the maximum values of residual sugar in good wine seem to be very high, possibly indicating an outlier or a high tail on the right side. Despite this high value, the interquartile range (IQR) of good wine is lower than that of bad wine, supporting the fact that the IQR is a variability measure highly robust to outliers.

Despite the presence of the outlier, we can observe that the mean residual sugar of bad wine is higher than that of good wine. The median also appears to be considerably higher. However, the question arises whether these differences are statistically significant.

+   Generate boxplots for both samples, placing them into one graphic. What do you observe?
```{r boxplot}

p <- ggplot(white.wine.quality, aes(good, residual.sugar))
p + geom_boxplot()

```

We can support the pattern we encountered in the previous question. It is highly likely that the value 65.8 is an outlier. There are a couple more values above the upper whisker. The median of the bad wine group is slightly higher than that of the good wine group. 

+   Generate a QQ-plot to compare two groups. Make sure to choose the same range for both axes. Add a y = x line to the plots. Comment on the results.

```{r qqplot}

range_values <- range(c(good.wine$residual.sugar, bad.wine$residual.sugar))

# Generate QQ-plot for good quality wine with same axis range
#ggplot(data = good.wine, aes(sample = residual.sugar)) +
#  geom_qq() +
#  geom_abline(slope = 1, intercept = 0, color = "red") +
#  xlim(range_values) + ylim(range_values) +
#  labs(title = "QQ-Plot for Residual Sugar in Good Quality White Wine")
  
# Generate QQ-plot for bad quality wine with same axis range
#ggplot(data = bad.wine, aes(sample = residual.sugar)) +
#  geom_qq() +
#  geom_abline(slope = 1, intercept = 0, color = "red") +
#  xlim(range_values) + ylim(range_values) +
#  labs(title = "QQ-Plot for Residual Sugar in Bad Quality White Wine")
bad.wine.sorted <- sort(bad.wine$residual.sugar)
good.wine.sorted.bwquantiles <- sort(quantile(good.wine$residual.sugar,
                           probs=ppoints(bad.wine$residual.sugar)))

range_values <- range(c(bad.wine.sorted, good.wine.sorted.bwquantiles))
plot(bad.wine.sorted,good.wine.sorted.bwquantiles,
     xlim = range_values, ylim = range_values, abline(0,1))

```

We can observe that the residual sugar distribution of good wine is more right-skewed than that of bad wine. Additionally, we can identify those extreme values in the distribution of good wines.

+   Plot the empirical distribution functions of both groups in one graphic. Use different
styles and add a legend. Interpret the results.

```{r ecdf}
plot(ecdf(good.wine$residual.sugar), verticals=TRUE, do.points=FALSE, main = "ECDFs of Residual Sugar")
plot(ecdf(bad.wine$residual.sugar), verticals=TRUE, do.points=FALSE, add=TRUE, col='brown'
     , lty=2)
legend("bottomright", legend = c("Good Quality", "Bad Quality"),
       col = c("black", "brown"), lty = c(1, 2))
```

While the brown plot represents the ECDF of residual sugar for bad wine, the black one represents that of good wine. From the ECDF, we can identify where the most values occur. It's evident that there are more values for good wine than for bad wine. However, up to a certain point, the number of good and bad wines is similar. Therefore, for lower values of residual sugar, we can say they distribute similarly.

3.  Consider now volatile.acidity for good and bad wines. Use boxplots, histograms, QQplots, summary statistics and empirical distribution functions to compare this variable for
good and bad wines. Comment on the results.

```{r volatile.acidity}
p <- ggplot(white.wine.quality, aes(good, volatile.acidity))
p + geom_boxplot()

hist(good.wine$volatile.acidity,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Volatile Acidity of Good Wine with Sturges' Rule",
 xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Volatile Acidity of Bad Wine with Sturges' Rule",
 xlab = "Volatile Acidity")


hist(good.wine$volatile.acidity,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Volatile Acidity of Good Wine with Scott's Rule",
     xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Volatile Acidity of Bad Wine with Scott's Rule",
     xlab = "Volatile Acidity")


hist(good.wine$volatile.acidity,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Volatile Acidity of Good Wine with Freedman-Diaconis Rule",
     xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Volatile Acidity of Bad Wine with Freedman-Diaconis Rule",
     xlab = "Volatile Acidity")


bad.wine.sorted <- sort(bad.wine$volatile.acidity)
good.wine.sorted.bwquantiles <- sort(quantile(good.wine$volatile.acidity,
                           probs=ppoints(bad.wine$volatile.acidity)))

range_values <- range(c(bad.wine.sorted, good.wine.sorted.bwquantiles))

plot(bad.wine.sorted,good.wine.sorted.bwquantiles,
     xlim = range_values, ylim = range_values, abline(0,1))

white.wine.quality %>%
  group_by(good) %>%
  summarise(mean.va = mean(volatile.acidity),
                        median.va = median(volatile.acidity),
                        sd.va = sd(volatile.acidity),
                        IQR.va = IQR(volatile.acidity),
                        min.va = min(volatile.acidity),
                        max.va = max(volatile.acidity))

plot(ecdf(good.wine$volatile.acidity), verticals=TRUE, do.points=FALSE, main = "ECDFs of Volatile Acidity")
plot(ecdf(bad.wine$volatile.acidity), verticals=TRUE, do.points=FALSE, add=TRUE, col='brown'
     , lty=2)
legend("bottomright", legend = c("Good Quality", "Bad Quality"),
       col = c("black", "brown"), lty = c(1, 2))

```

Boxplots of both wine groups seem to have a very heavy and long right tail. The median of the bad wine group is slightly higher than that of the good wine group.

When we look at histograms, transitioning from Sturges' Rule to Scott's Rule, the number of bins increases for both good and bad wines. The reason why it did not differ in residual sugar might be that the variance in volatile acidity is much lower than the variance in residual sugar. Additionally, the IQR of volatile acidity is significantly lower than that of residual sugar in both wines. Eventually, the bin width of Freedman-Diaconis is very small in bad wines as well.

Looking at the QQ plot, we can see that the values of good wine are more concentrated or have less variance than those for bad wines because of the slope of the QQ plot. Additionally since values are below the y=x line, the good wine group is expected to have lower mean.

On the ECDF plot, we can see that there are more values for good wine than for bad wine. This is also consistent with the fact that there were more bad wine values below the good wine values on the QQ plot.

## Examine the distribution of data

#### Dataset

Consider the dataset from the previous exercise and its variable pH.

#### Exercises

1- Plot a histogram of pH for all wines and add to the plot a normal density, estimating the parameters from the data. Produce same histograms with corresponding normal densities for good and bad wines separately. Do you observe any differences in the distributions?

```{r hist.FreedmanDiaconis.pH}

hist(good.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for Good Wine with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(good.wine$pH), sd = sd(good.wine$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

hist(bad.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for Bad Wine with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(bad.wine$pH), sd = sd(bad.wine$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

```

Distributions seem more or less normal and similar. The pH of bad wine is slightly spykier than that of the pH of good wine. 

2- Generate QQ-plots of pH for good, bad and all wines to compare empirical quantiles of the samples to the theoretical quantiles of a normal distribution. Produce PP-plots for all three datasets. Comment on the differences between QQ-plots and PP-plots. Do you think all samples follow a normal distribution?

```{r qqplot.theoretical}
qqnorm(good.wine$pH, main="QQ-plot of pH for Good Wine")
qqline(good.wine$pH)

qqnorm(bad.wine$pH, main="QQ-plot of pH for Bad Wine")
qqline(bad.wine$pH)

qqnorm(white.wine.quality$pH, main="QQ-plot of pH for All Wines")
qqline(white.wine.quality$pH)

```

Good wine and all wine datasets seem to have a lighter left tail and heavier right tail than the theoretical normal distribution. Bad wine seems to only have heavier right tail but left tail seems to be more or less similar to the normal distribution.

```{r ppplot.theoretical}

standardized.data <- (good.wine$pH - mean(good.wine$pH)) / sd(good.wine$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="QQ-plot of pH for Good Wine")
abline(0,1)

standardized.data <- (bad.wine$pH - mean(bad.wine$pH)) / sd(bad.wine$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="QQ-plot of pH for Bad Wine")
abline(0,1)

standardized.data <- (white.wine.quality$pH - mean(white.wine.quality$pH)) / sd(white.wine.quality$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="QQ-plot of pH for All Wine")
abline(0,1)

```

The only difference between QQ and PP plot is that considering ECDFs on the same plot, whilst QQ-plot plots the corresponding values horizontally, PP-plot plots the vertical values. Therefore it makes more sense that tails are better expressed on qq-plot and mode is better expressed with pp-plot. Differences on tails are more clear on QQ-plots. But they more or less follow normal distributions.


3-) Plot the empirical distribution functions Fn for all three datasets. Add the pointwise confidence bands for α = 0.05 using the central limit theorem and Slutzky’s lemma (ensure that
the confidence bands are in [0; 1]).

```{r ecdf.pwbands}
ecdf.func <- function(data) { 
  Length <- length(data) 
  sorted <- sort(data) 
  
  ecdf <- rep(0, Length) 
  for (i in 1:Length) { 
    ecdf[i] <- sum(sorted <= data[i]) / Length 
  } 
  return(ecdf) 
}
ecdf.pwbands <- function(data, title){
  Fn <- ecdf(data)
  ecdf_data <- ecdf.func(data)
  
  z <- sqrt(ecdf_data*(1-ecdf_data) / length(ecdf_data))
  
  plot(Fn, col="darkgreen", lwd=2, main = title)
  
  upper_bound <- Fn(sort(data)) + 1.96 * z
  lower_bound <- Fn(sort(data)) - 1.96 * z
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  lines(sort(data), upper_bound, col="salmon", lwd=2)
  lines(sort(data), lower_bound, col="salmon", lwd=2)
}

ecdf.pwbands(good.wine$pH, "Good Wine pH ECDF and Pointwise Confidence Intervals")
ecdf.pwbands(bad.wine$pH, "Bad Wine pH ECDF and Pointwise Confidence Intervals")
ecdf.pwbands(good.wine$pH, "All Wine pH ECDF and Pointwise Confidence Intervals")

```

4- Plot the empirical distribution functions Fn for all three datasets together with the uniform
confidence bands for α = 0.05. Compare to the bands obtained in (3).

```{r ecdf.ksbands}
ecdf.ksbands <- function(data, title){
  Fn <- ecdf(data)
  ecdf_data <- ecdf.func(data)

  plot(Fn, col="darkgreen", lwd=2, main = title)
  
  D <- 1.3581/ sqrt(length(data))
  upper_bound <- Fn(sort(data)) + D
  lower_bound <- Fn(sort(data)) - D
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  lines(sort(data), upper_bound, col="salmon", lwd=2)
  lines(sort(data), lower_bound, col="salmon", lwd=2)
}

ecdf.ksbands(good.wine$pH, "Good Wine pH ECDF and Uniform Confidence Intervals")
ecdf.ksbands(bad.wine$pH, "Bad Wine pH ECDF and Uniform Confidence Intervals")
ecdf.ksbands(good.wine$pH, "All Wine pH ECDF and Uniform Confidence Intervals")

```

Kolmogorov-Smirnov bandwidth for sample size over 35 with 0.05 alpha value is [1.36/sqrt(N)](https://real-statistics.com/statistics-tables/kolmogorov-smirnov-table/) = 0.02382667. The problem with pointwise confidence intervals is, that if we compute them for a lot of points, we accumulate errors. Uniform confidence bands are slightly wider. 

5- Plot the empirical distribution functions of pH for good and bad wines together with the uniform confidence bands in one plot. What can you conclude from this plot?

```{r ecdf.ksbands.good.bad}
ecdf.ksbands <- function(data, title, col){
  Fn <- ecdf(data)
  
  D <- 1.3581/ sqrt(length(data))
  upper_bound <- Fn(sort(data)) + D
  lower_bound <- Fn(sort(data)) - D
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  lines(sort(data), upper_bound, col=col, lwd=2, lty = "dashed")
  lines(sort(data), lower_bound, col=col, lwd=2, lty = "dashed")
}

# Create an empty plot with x-axis limits covering the range of both datasets
plot(ecdf(good.wine$pH), col="darkgreen", lwd=2, main = "Wine pH ECDF and Uniform Confidence Intervals", xlim = range(c(good.wine$pH, bad.wine$pH)))

# Plot ECDFs for both good and bad wine pH
lines(ecdf(bad.wine$pH), col="darkred", lwd=2)

# Plot confidence intervals for both good and bad wine pH
ecdf.ksbands(good.wine$pH, "Good Wine pH", col="blue")
ecdf.ksbands(bad.wine$pH, "Bad Wine pH", col="orange")

# Add a legend
legend("bottomright", legend=c("Good Wine (ECDF)", "Bad Wine (ECDF)", "Good Wine (Confidence Intervals)", "Bad Wine (Confidence Intervals)"), 
       col=c("darkgreen", "darkred", "blue", "orange"), lwd=2, cex=0.8, lty=c("solid", "solid", "dashed", "dashed"))

```

On the ECDF we can see that up to a certain point, about to the pH value 3.1, good and bad wine pHs follow a similar distribution. Afterwards the gap increases as there are less bad wines with higher pH values. Additionally, bad wines seem to have a larger confidence bands on tails compared to good wines.

## Maximum likelihood Estimation
Let X be Laplace distributed with parameters (µ, σ)
t ∈ R × (0, ∞), i.e., X has density

\[ f(x | \mu, σ) = \frac{1}{2σ} \exp\left(-\frac{|x - \mu|}{σ}\right) \]

#### Exercises
1- Consider a sample of independent observations (X1, . . . , Xn). Show that the maximum likelihood estimator for µ is the median of the sample. Is it unique?


Check out this informative [blog](https://math.stackexchange.com/questions/240496/finding-the-maximum-likelihood-estimator
).

Consider the likelihood function for N data samples:
\[L(\mu,\sigma;x) = \prod_{t=1}^{N} \frac{1}{2\sigma} e^{-\frac{|x_t-\mu|}{\sigma}} = (2\sigma)^{-N} e^{-\frac{1}{\sigma}\sum_{t=1}^{N}|x_t-\mu|}\]

Take the log likelihood function:

\[l(\mu,\sigma;x) = -N \ln(2\sigma) - \frac{{1}}{\sigma} \sum_{t=1}^{N}|x_t-\mu|\]


Take the derivative with respect to the parameter $\mu$:
\[ 
\frac{\partial l}{\partial \mu} = -\frac{1}{\sigma} \sum_{t=1}^{N} \frac{\partial |x_t-\mu|}{\partial \mu}
\]

which is equal to:
\[
\frac{1}{\sigma} \sum_{t=1}^{N} \text{sgn}(x_t-\mu) = 0 \quad (1)
\]

The sign function, usually denoted as $\text{sgn}(x)$, returns:
\[
\text{sgn}(x) = 
\begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x = 0 \\
-1 & \text{if } x < 0 
\end{cases}
\]

The median is a solution for both cases when N is an odd or even number. However, it is not unique for the even case. If N is even, {\mu} can take values strictly between $> x_{\left(\frac{n}{2}\right)}$ and $< x_{\left(\frac{n}{2}+1\right)}$. If it is odd it is unique and is equal to the median.

2- Generate n = 20 independent realisations of X with µ = 1 and σ = 1. Determine the maximum likelihood estimator of µ based on this sample using R function quantile. Function quantile has 9 types of sample quantiles. Experiment with the different types of quantiles that are most suitable for the data (justify). Are there any differences? Increase now the sample to n = 1000 and compare different median estimators. Comment on the result.

```{r laplace.quantile}
# install.packages('jmuOutlier')
library(jmuOutlier)
set.seed(123)

# Set parameters
mu <- 1
sigma <- 1

# Generate 20 independent realizations
n <- 20
laplace.data.20 <- rlaplace(n, mu, sigma)

for (i in 1:9) {
  cat("20 Samples Type ", i, " Median: ", quantile(laplace.data.20, 0.5, type = i), "\n")
}


n <- 1000
laplace.data.1000 <- rlaplace(n, mu, sigma)

for (i in 1:9) {
  cat("1000 Samples Type ", i, " Median: ", quantile(laplace.data.1000, 0.5, type = i), "\n")
}

```

To generate 20 independent realizations of a Laplace distribution with parameters µ = 1 and σ = 1 in R, you can use the rlaplace() function from the jmuOutlier package. They result in the same pattern. Type 1,3 and 4 are the same and the rest are another value. The reason for this is that for types 5 through 9, a different version of linear interpolation is applied. Type 4 is the linear interpolation of the empirical cdf. Type 2 averages at discontinuities and since we have even sample sizes and we are interested in the median, they all result in the same value. Therefore, type 1,3 and 4 rounds down and the rest takes the average of the 10th and 11th or the 500th and the 501st. In our case, type 2,5,6,7,8 and 9 can be used for an unbiased estimator.
When comparing the results for n = 20 and n = 1000, you'll notice that the medians are very close. This is expected because as the sample size increases, the sample median tends to converge to the population median, resulting in more consistent estimates regardless of the quantile type used.

3- Write your own function that calculates the maximum likelihood estimator for a Laplace sample numerically using R function optimise. Describe how R function optimise finds
the maximum. Can you employ a Newton-Raphson algorithm for this problem? Generate n = 20 and n = 1000 independent realisations of X with µ = 1 and σ = 1. Calculate the maximum likelihood estimators based on both samples with your function and using quantile. Compare both estimators, comment on the results.

```{r laplace}

laplace_data <- laplace.data.1000
  laplace.log.likelihood <- function(mu) {
    (-length(laplace_data)*log(2 *sigma) -(1/sigma) * sum(abs(laplace_data - mu)))
  }

optimise(laplace.log.likelihood,maximum = T,
         lower = 0, upper = 10)

laplace_data <- laplace.data.20
optimise(laplace.log.likelihood,maximum = T,
         lower = 0, upper = 10)



laplace_data <- laplace.data.1000

# Newton-Raphson
niter <- 100
# xk Initial values
laplace.log.likelihood.newton <- function(xk){
  h <- 0.01
  for(k in 1:niter){
    Q <- (h/2) * ((laplace.log.likelihood(xk + h) - laplace.log.likelihood(xk - h))/
                    (laplace.log.likelihood(xk + h) - 2*laplace.log.likelihood(xk) + laplace.log.likelihood(xk - h)))
    xnew <- xk - Q
    print(xnew)
    xk <- xnew
  }
}

laplace.log.likelihood.newton(0.21)


laplace_data <- laplace.data.20
laplace.log.likelihood.newton(0.21)

```

According to the [documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optimize), optimise() method uses somewhat a combination of golden section search and successive parabolic interpolation. The algorithm starts with an initial interval and evaluates the function at two points within this interval. The points are chosen according to the golden section ratio $φ = (1 + √5) / 2$, which ensures that the interval contracts efficiently.  After the initial evaluations, the algorithm performs successive parabolic interpolations to refine the location of the maximum. This technique utilizes information from previous function evaluations to approximate the maximum more accurately. We get very close optimum results with the corresponding data.
Check out this informative video on YouTube: [YouTube Video](https://www.youtube.com/watch?v=8z4I348eayg).


Newton-Raphson method:
\[x_{k+1} = x_k - Q_k\]
\[Q_k = \frac{f'(x_k)}{f''(x_k)}\]

The method tends to converge if the following condition holds true:
\[|f(x)f''(x)| < |f'(x)|^2\]

Using the central difference approximation for the derivative:
\[Q_k = \frac{h}{2} \left[ \frac{f(x + h) - f(x - h)}{f(x + h) - 2f(x) + f(x - h)} \right]\]

For a small sample size like n=20, even though the log-likelihood function tries to smooth out, it can be highly irregular due to the limited data. This irregularity can cause the Newton-Raphson method (or even optimise) to perform poorly. Both performs well with the sample size 1000 however, newton-raphson does not converge.

4-) Let us now study the distribution of the maximum likelihood estimator. For this, calculate M = 5000 maximum likelihood estimators of µ based on the sample of n = 20 random
variables generated from the Laplace distribution with µ = 1 and σ = 1. Repeat the same for the sample size n = 1000. Use histograms and QQ-plots to check if both Monte Carlo samples follow a normal distribution. Compare variances of both distributions, comment on the results.

```{r}
# Function to find MLE for mu using optimise
find_mle_laplace <- function(x) {
  result <- optimise(function(mu) laplace.log.likelihood(mu), maximum = TRUE, lower = -10, upper = 10)
  return(result$maximum)
}

# Newton-Raphson method
laplace.log.likelihood.newton <- function(xk){
  h <- 0.01
  tol = 1e-6
  for(k in 1:niter){
    Q <- (h/2) * ((laplace.log.likelihood(xk + h) - laplace.log.likelihood(xk - h))/
                    (laplace.log.likelihood(xk + h) - 2*laplace.log.likelihood(xk) + laplace.log.likelihood(xk - h)))
    xnew <- xk - Q
    
    if(is.na(xnew)){
      return("No Convergence")
    }
    
    if (abs(xnew - xk) < tol) {
      return(xnew)  # Convergence check
    }
    xk <- xnew
  }
  return(xk)
}

# Parameters
mu <- 1
sigma <- 1
n <- 20
M <- 5000

# Storage for results
results_optimise <- numeric(M)
results_quantile <- numeric(M)
results_newton <- numeric(M)

for (i in 1:M) {
  laplace_data <- rlaplace(n, mu, sigma)
  
  # Quantile (median)
  quantile_result <- quantile(laplace_data, 0.5, type = 5)
  results_quantile[i] <- quantile_result
  
  # MLE using optimise
  optimise_result <- find_mle_laplace(laplace_data)
  results_optimise[i] <- optimise_result
  
  # MLE using Newton-Raphson
  newton_result <- laplace.log.likelihood.newton(0.21)
  results_newton[i] <- newton_result
}

# Display results
cat("First 5 results using optimise:\n", head(results_optimise, 5), "\n")
cat("First 5 results using quantile:\n", head(results_quantile, 5), "\n")

# Plot histograms and QQ-plots
par(mfrow = c(3, 2))
hist(results_optimise, main = "Histogram of MLEs (optimise) for n = 20", xlab = "MLE of mu", breaks = 50)
qqnorm(results_optimise, main = "QQ-plot of MLEs (optimise) for n = 20")
qqline(results_optimise)

hist(results_quantile, main = "Histogram of MLEs (quantile) for n = 20", xlab = "MLE of mu", breaks = 50)
qqnorm(results_quantile, main = "QQ-plot of MLEs (quantile) for n = 20")
qqline(results_quantile)

# Variance comparison
variance_optimise <- var(results_optimise)
variance_quantile <- var(results_quantile)

cat("Variance of MLEs using optimise for n = 20:", variance_optimise, "\n")
cat("Variance of MLEs using quantile for n = 20:", variance_quantile, "\n")

# n 1000
n = 1000

for (i in 1:M) {
  laplace_data <- rlaplace(n, mu, sigma)
  
  # Quantile (median)
  quantile_result <- quantile(laplace_data, 0.5, type = 5)
  results_quantile[i] <- quantile_result
  
  # MLE using optimise
  optimise_result <- find_mle_laplace(laplace_data)
  results_optimise[i] <- optimise_result
  
  # MLE using Newton-Raphson
  newton_result <- laplace.log.likelihood.newton(0.3)
  results_newton[i] <- newton_result

}
results_newton <- subset(results_newton, as.numeric(results_newton) >= 0.8 &
                          as.numeric(results_newton) <= 1.2)


results_newton <- as.numeric(results_newton)


# Display results
cat("First 5 results using optimise:\n", head(results_optimise, 5), "\n")
cat("First 5 results using quantile:\n", head(results_quantile, 5), "\n")
cat("First 5 results using Newton-Raphson:\n", head(results_newton, 5), "\n")

# Plot histograms and QQ-plots
par(mfrow = c(3, 2))
hist(results_optimise, main = "Histogram of MLEs (optimise) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_optimise, main = "QQ-plot of MLEs (optimise) for n = 1000")
qqline(results_optimise)

hist(results_quantile, main = "Histogram of MLEs (quantile) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_quantile, main = "QQ-plot of MLEs (quantile) for n = 1000")
qqline(results_quantile)

hist(results_newton, main = "Histogram of MLEs (Newton-Raphson) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_newton, main = "QQ-plot of MLEs (Newton-Raphson) for n = 1000")
qqline(results_newton)

# Variance comparison
variance_optimise <- var(results_optimise)
variance_quantile <- var(results_quantile)
variance_newton <- var(results_newton)

cat("Variance of MLEs using optimise for n = 1000:", variance_optimise, "\n")
cat("Variance of MLEs using quantile for n = 1000:", variance_quantile, "\n")
cat("Variance of MLEs using Newton-Raphson for n = 1000:", variance_newton, "\n")
```

## Linear Regression

#### Dataset
Go to Kaggle.com and download the data on house prices. This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. There are altogether 19 variables, but we will consider in the analysis only the following ones:

-   price: Price
-   bedrooms: Number of bedrooms
-   bathrooms: Number of bathrooms per bedroom
-   sqft_living: Square footage of the home
-   floors: Total floors in house
-   view: Has been viewed (1 for viewed; 0 for not viewed)
-   condition: How good is the condition (from 1 to 5)
-   grade: Grade given to the housing unit based on King County grading system (from 1 to 13)
-   yr_built: Year the house was built

```{r}
kc_house_data <- read.csv("archive/kc_house_data.csv")

kc_house_data <- kc_house_data %>%
  select(price, bedrooms, bathrooms,
         sqft_living,
         floors,
         view,
         condition,
         grade,
         yr_built)

head(kc_house_data)
```

#### Exercises

1-) Estimate a linear model with the response variable price and all remaining variables as covariates. Are all variables significant? How large is R2 and how can this be interpreted?
Perform the residual analysis to validate the model. Are there any departures from the linear regression model assumptions?

```{r}
model <- lm(price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = kc_house_data)

summary(model)
```

The estimated effect of each predictor on the response variable (price). For instance, each additional bedroom decreases the price by approximately 40,650 dollars, while each additional bathroom increases it by about 47,690 dollars. Pr(>|t|): The p-value associated with the t-statistic, testing the null hypothesis that the coefficient is equal to zero (no effect). A lower p-value (typically < 0.05) indicates that the predictor is statistically significant. All variables here are highly significant (p-values < 2e-16), suggesting strong evidence against the null hypothesis. 
R-squared, the proportion of variance in the response variable explained by the predictors. Here, 63.59% of the variance in price is explained by the model. Adjusted R-squared is adjusted for the number of predictors in the model, providing a more accurate measure when multiple predictors are used.
The summary indicates a strong model fit, with all predictors being statistically significant and the model explaining a substantial portion of the variance in house prices. However, the relatively high residual standard error suggests that while the model captures a significant portion of the variability, there is still considerable unexplained variation in house prices.

```{r}
par(mfrow = c(2, 2))
plot(model)
```
On the Residuals vs Fitted plot, homoscedasticity of ordinary least squares estimation is clearly violated. Variance in residuals increases as fitted values increase. We can additionally support the violation of homoscedasticity by looking at the crookedness of Scale-Location plot. Q-Q plot of residuals is violated too. There are strong lower and upper tails. On the Residuals vs Leverage plot, even though there are points that are close to boundaries, most of them are within the range. So we can say that there are not many influential or outlier cases in linear regression analysis.

2-) Produce a histogram and a QQ-plot of the response variable price, as well as of its logtransform log(price). Compare both distributions to the normal one. Fit now a linear model with the response variable log(price). Compare the estimated model with the one from (a) in terms of R2, significance and effect of covariates and model fit (via residual analysis). Which model is more adequate?

```{r}

kc_house_data$log_price <- log(kc_house_data$price)

par(mfrow = c(2, 2))
# Histogram of price
hist(kc_house_data$price, main = "Histogram of Price", xlab = "Price", breaks = 30)

# QQ-plot of price
qqnorm(kc_house_data$price, main = "QQ-Plot of Price")
qqline(kc_house_data$price)

# Histogram of log(price)
hist(kc_house_data$log_price, main = "Histogram of Log(Price)", xlab = "Log(Price)", breaks = 30)

# QQ-plot of log(price)
qqnorm(kc_house_data$log_price, main = "QQ-Plot of Log(Price)")
qqline(kc_house_data$log_price)

```

Both the Histogram and QQ-Plot of price shows a very high right tail. Log(price) still has a slightly heavy tail but it is considerably low.

```{r}
# Fit the linear model with log(price) as response
model_log_price <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = kc_house_data)

# Summary of the model
summary(model_log_price)
```

The R-squared values are slightly higher in the log model, indicating that this model explains a bit more variance in the response variable compared to the original model. In both models, all predictors are statistically significant with p-values < 2e-16. The t-values for the predictors are generally higher in the log model, suggesting stronger relationships between predictors and the log-transformed response variable. In the log transformed models, the interpretation of coefficients is slightly different. Following the bathroom independent variable, each unit of increase now means, the dependent variable is multiplied by $e^(169.3)$.

```{r}
par(mfrow = c(2, 2))
plot(model_log_price)
```

Residuals vs Fitted and Scale-Location plots shows that homoscedasticity problem is highly resolved. Q-Q Plot also follows a straight line. And we cannot even see the most of dashed lines in Residuals vs Leverage. The point 12778 seems pretty close to the one below tho.

In summary, the model with log(price) as the response variable appears to be more appropriate as it:

-   Has a higher R-squared value, indicating a better fit.
-   Shows stronger relationships between predictors and the response variable.
-   Demonstrates improved residual distribution, suggesting fewer outliers and a more consistent error structure.

3-) In the model from (b) interpret the effect of each covariate on the response. Plot each covariate against log(price). Is the assumption of the linear dependence between covariates and
response plausible for all covariates? Add to the model from (b) squared terms for yr_built
and sqft_living. Are these terms signifficant? Does adding these two terms improve the
model fit in terms of R2?

```{r , warning=FALSE, message=FALSE}
covariates <- c("bedrooms", "bathrooms", "sqft_living", "floors", "view", "condition", "grade", "yr_built")

# Create plots
for (var in covariates) {
  p <- ggplot(kc_house_data, aes_string(x = var, y = "log_price")) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", col = "blue") +
    labs(title = paste("Log(price) vs", var), x = var, y = "Log(price)")
  print(p)
}
```


References

https://math.stackexchange.com/questions/240496/finding-the-maximum-likelihood-estimator

https://www.youtube.com/watch?v=8z4I348eayg