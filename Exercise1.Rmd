---
title: "Exercise 1"
output: html_document
date: "2024-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sample Statistics

#### Dataset

Go to UCI Machine learning repository and download the data on the white wine quality. This page contains also the background information on the data. In our analysis we will only consider the following variables:

-   volatile.acidity: Volatile acidity

-   residual.sugar: Residual sugar

-   pH: pH level

-   quality: Wine quality in a score between 0 and 10

#### Exercises

1. Read the data into R. Add to the data frame a new binary variable good which is 1 if quality \> 5 and 0 otherwise. We would like to compare volatile.acidity and residual.sugar for good and bad wines.

```{r a, warning=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(ggplot2)
library(jmuOutlier)
library(ISLR)
library(glmnet)
library(faraway)
library(JoSAE)
library(lme4)
library(nlme)
library(astsa)
library(maps)
library(mapdata)

white.wine.quality <- read.csv("winequality-white.csv", sep = ";")
white.wine.quality <- white.wine.quality %>% select(volatile.acidity, residual.sugar, 
                              pH, quality)

white.wine.quality <- white.wine.quality %>% 
  mutate(good = quality > 5)

head(white.wine.quality)
```

2. First consider variable residual.sugar.

+ Plot histograms of residual.sugar for good and bad wines using different methods available in R to choose the bin width. Comment on the shape of both histograms and differences in distribution, if any.

```{r hist.Sturges}

good.wine <- white.wine.quality[white.wine.quality$good == 1,]
bad.wine <- white.wine.quality[white.wine.quality$good == 0,]

hist(good.wine$residual.sugar,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Residual Sugar of Good Wine with Sturges' Rule",
 xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Residual Sugar of Bad Wine with Sturges' Rule",
 xlab = "Residual Sugar")
```

There are 1640 bad wine and 3258 good wine rows. The distribution of good wine seems to have a long tail on the right. The number of bins for good wine is 14 and it is 12 for bad wine when Sturges formula is used.

```{r hist.ScottsChoice}
hist(good.wine$residual.sugar,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Residual Sugar of Good Wine with Scott's Rule",
     xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Residual Sugar of Bad Wine with Scott's Rule",
     xlab = "Residual Sugar")
```

When Scott's choice is used, the number of breaks remains the same for the bad wine, but it increases significantly for the good wine. This allows us to capture more information, but it also introduces more noise.

```{r hist.FreedmanDiaconis}
hist(good.wine$residual.sugar,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Residual Sugar of Good Wine with Freedman-Diaconis Rule",
     xlab = "Residual Sugar")

hist(bad.wine$residual.sugar,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Residual Sugar of Bad Wine with Freedman-Diaconis Rule",
     xlab = "Residual Sugar")
```

When Freedman-Diaconis' choice is used, the number of breaks is the same as with Scott's choice. Both Freedman-Diaconis' and Scott's choices take variability into account. However, Freedman-Diaconis' choice utilizes the interquartile range, which can make it more robust to outliers.

+   Calculate the summary statistics for both wine groups, that is: mean, median, standard deviation, interquartile range, minimum and maximum of both samples. Display the results in a table and comment on the differences between both groups, if any.


```{r summary}
white.wine.quality %>%
  group_by(good) %>%
  summarise(mean.rs = mean(residual.sugar),
                        median.rs = median(residual.sugar),
                        sd.rs = sd(residual.sugar),
                        IQR.rs = IQR(residual.sugar),
                        min.rs = min(residual.sugar),
                        max.rs = max(residual.sugar))

```
Considering that both of these variables from the good and bad groups are bounded by 0, the maximum values of residual sugar in good wine seem to be very high, possibly indicating an outlier or a high tail on the right side. Despite this high value, the interquartile range (IQR) of good wine is lower than that of bad wine, supporting the fact that the IQR is a variability measure highly robust to outliers.

Despite the presence of the outlier, we can observe that the mean residual sugar of bad wine is higher than that of good wine. The median also appears to be considerably higher. However, the question arises whether these differences are statistically significant.

+   Generate boxplots for both samples, placing them into one graphic. What do you observe?
```{r boxplot}

p <- ggplot(white.wine.quality, aes(factor(good, levels = c(FALSE, TRUE), labels = c("Bad", "Good")), residual.sugar))
p + geom_boxplot()

```

We can support the pattern we encountered in the previous question. It is highly likely that the value 65.8 is an outlier. There are a couple more values above the upper whisker. The median of the bad wine group is slightly higher than that of the good wine group. 

+   Generate a QQ-plot to compare two groups. Make sure to choose the same range for both axes. Add a y = x line to the plots. Comment on the results.

```{r qqplot}

qqplot(good.wine$residual.sugar, bad.wine$residual.sugar, main = "QQ-plot: Good vs Bad Wines",
       xlab = "Residual Sugar - Good Wines", ylab = "Residual Sugar - Bad Wines")
abline(0, 1, col = "red")  # Add y = x line
```

We can observe that the residual sugar distribution of good wine is more right-skewed than that of bad wine. Additionally, we can identify those extreme values in the distribution of good wines.

+   Plot the empirical distribution functions of both groups in one graphic. Use different
styles and add a legend. Interpret the results.

```{r ecdf}
ecdf.func <- function(data) { 
  Length <- length(data) 
  sorted <- sort(data) 
  
  ecdf <- rep(0, Length) 
  for (i in 1:Length) { 
    ecdf[i] <- sum(sorted <= sorted[i]) / Length 
  } 
  return(list(sorted = sorted, ecdf = ecdf))
}

# Calculate ECDFs for good and bad wine residual sugar
ecdf.good.rs <- ecdf.func(good.wine$residual.sugar)
ecdf.bad.rs <- ecdf.func(bad.wine$residual.sugar)

# Plot ECDF for good wine
plot(ecdf.good.rs$sorted, ecdf.good.rs$ecdf, type = "s", main = "ECDFs of Residual Sugar", xlab = "Residual Sugar", ylab = "ECDF", col = "black", lty = 1)

# Add ECDF for bad wine
lines(ecdf.bad.rs$sorted, ecdf.bad.rs$ecdf, type = "s", col = "brown", lty = 2)

# Add legend
legend("bottomright", legend = c("Good Quality", "Bad Quality"), col = c("black", "brown"), lty = c(1, 2))
```

Here the definition of ECDF is important. I will simply use the one in the lecture in the rest of the report.
While the brown plot represents the ECDF of residual sugar for bad wine, the black one represents that of good wine. From the ECDF, we can identify where the most values occur. It's evident that there are more values of lower residual sugar for good wine than for bad wine. However, up to a certain point, we can say they distribute similarly. Afterwards, bad quality wines usually have more sugar.

3.  Consider now volatile.acidity for good and bad wines. Use boxplots, histograms, QQplots, summary statistics and empirical distribution functions to compare this variable for
good and bad wines. Comment on the results.

```{r volatile.acidity}
p <- ggplot(white.wine.quality, aes(good, volatile.acidity))
p + geom_boxplot()

hist(good.wine$volatile.acidity,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Volatile Acidity of Good Wine with Sturges' Rule",
 xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
 breaks = "Sturges", 
 freq=F,
 main = "Histogram of Volatile Acidity of Bad Wine with Sturges' Rule",
 xlab = "Volatile Acidity")


hist(good.wine$volatile.acidity,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Volatile Acidity of Good Wine with Scott's Rule",
     xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
     breaks = "Scott", 
     freq=F,
     main = "Histogram of Volatile Acidity of Bad Wine with Scott's Rule",
     xlab = "Volatile Acidity")


hist(good.wine$volatile.acidity,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Volatile Acidity of Good Wine with Freedman-Diaconis Rule",
     xlab = "Volatile Acidity")

hist(bad.wine$volatile.acidity,
     breaks = "Freedman-Diaconis", 
     freq=F,
     main = "Histogram of Volatile Acidity of Bad Wine with Freedman-Diaconis Rule",
     xlab = "Volatile Acidity")


qqplot(bad.wine$volatile.acidity, good.wine$volatile.acidity, main = "QQ-plot: Good vs Bad Wines",
       xlab = "Volitile Acidity - Bad Wines", ylab = "Volitile Acidity - Good Wines")
abline(0, 1, col = "red")  # Add y = x line

white.wine.quality %>%
  group_by(good) %>%
  summarise(mean.va = mean(volatile.acidity),
                        median.va = median(volatile.acidity),
                        sd.va = sd(volatile.acidity),
                        IQR.va = IQR(volatile.acidity),
                        min.va = min(volatile.acidity),
                        max.va = max(volatile.acidity))


ecdf.good.va <- ecdf.func(good.wine$volatile.acidity)
ecdf.bad.va <- ecdf.func(bad.wine$volatile.acidity)

# Plot ECDF for good wine
plot(ecdf.good.va$sorted, ecdf.good.va$ecdf, type = "s", main = "ECDFs of Volatile Acidity", xlab = "Volatile Acidity", ylab = "ECDF", col = "black", lty = 1)

# Add ECDF for bad wine
lines(ecdf.bad.va$sorted, ecdf.bad.va$ecdf, type = "s", col = "brown", lty = 2)

# Add legend
legend("bottomright", legend = c("Good Quality", "Bad Quality"), col = c("black", "brown"), lty = c(1, 2))

```

Boxplots of both wine groups seem to have a very heavy and long right tail. The median of the bad wine group is slightly higher than that of the good wine group.

When we look at histograms, transitioning from Sturges' Rule to Scott's Rule, the number of bins increases for both good and bad wines. The reason why it did not differ in residual sugar might be that the variance in volatile acidity is much lower than the variance in residual sugar. Additionally, the IQR of volatile acidity is significantly lower than that of residual sugar in both wines. Eventually, the bin width of Freedman-Diaconis is very small in bad wines as well.

Looking at the QQ plot, we can see that the values of good wine are more concentrated or have less variance than those for bad wines because of the slope of the QQ plot. Additionally since values are below the y=x line, the good wine group is expected to have lower mean.

On the ECDF plot, we can see that there are more values of lower volatile acidity for good wine than for bad wine. This is also consistent with the fact that there were more bad wine values below the good wine values on the QQ plot.

## Examine the distribution of data

#### Dataset

Consider the dataset from the previous exercise and its variable pH.

#### Exercises

1- Plot a histogram of pH for all wines and add to the plot a normal density, estimating the parameters from the data. Produce same histograms with corresponding normal densities for good and bad wines separately. Do you observe any differences in the distributions?

```{r hist.FreedmanDiaconis.pH}

hist(good.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for All Wines with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(white.wine.quality$pH), sd = sd(white.wine.quality$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

hist(good.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for Good Wine with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(good.wine$pH), sd = sd(good.wine$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

hist(bad.wine$pH,
     freq = FALSE,
     main = "Histogram of pH for Bad Wine with Normal Density Estimation",
     xlab = "pH")
curve(dnorm(x, mean = mean(bad.wine$pH), sd = sd(bad.wine$pH)), 
      add = TRUE, 
      col = "blue", 
      lwd = 2)

```

Distributions seem more or less normal and similar. The pH of bad wine is slightly spikier than that of the pH of good wine. 

2- Generate QQ-plots of pH for good, bad and all wines to compare empirical quantiles of the samples to the theoretical quantiles of a normal distribution. Produce PP-plots for all three datasets. Comment on the differences between QQ-plots and PP-plots. Do you think all samples follow a normal distribution?

```{r qqplot.theoretical}
qqnorm(good.wine$pH, main="QQ-plot of pH for Good Wine")
qqline(good.wine$pH)

qqnorm(bad.wine$pH, main="QQ-plot of pH for Bad Wine")
qqline(bad.wine$pH)

qqnorm(white.wine.quality$pH, main="QQ-plot of pH for All Wines")
qqline(white.wine.quality$pH)

```

qqnorm() function plots the emprical distribution using ppoints() function in R. ppoints() uses (1:m - a)/(m + (1-a)-a) where m is either n, if length(n)==1, or length(n). And a by default a = if(n <= 10) 3/8 else 1/2).
Good wine and all wine datasets seem to have a lighter left tail and heavier right tail than the theoretical normal distribution. Bad wine seems to only have heavier right tail but left tail seems to be more or less similar to the normal distribution.

```{r ppplot.theoretical}

standardized.data <- (good.wine$pH - mean(good.wine$pH)) / sd(good.wine$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="PP-plot of pH for Good Wine")
abline(0,1)

standardized.data <- (bad.wine$pH - mean(bad.wine$pH)) / sd(bad.wine$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="PP-plot of pH for Bad Wine")
abline(0,1)

standardized.data <- (white.wine.quality$pH - mean(white.wine.quality$pH)) / sd(white.wine.quality$pH)
probDist <- pnorm(standardized.data)
plot(sort(probDist), ppoints(length(standardized.data)), 
     main="PP-plot of pH for All Wine")
abline(0,1)

```

The only difference between QQ and PP plot is that considering ECDFs on the same plot, whilst QQ-plot plots the corresponding values horizontally, PP-plot plots the vertical values. Therefore it makes more sense that tails are better expressed on qq-plot and mode is better expressed with pp-plot. Differences on tails are more clear on QQ-plots. But they more or less follow normal distributions.


3-) Plot the empirical distribution functions Fn for all three datasets. Add the pointwise confidence bands for α = 0.05 using the central limit theorem and Slutzky’s lemma (ensure that
the confidence bands are in [0; 1]).

```{r ecdf.pwbands.plot}
ecdf.pwbands.plot <- function(data, title){
  ecdf_data <- ecdf.func(data)
  
  z <- sqrt(ecdf_data$ecdf*(1-ecdf_data$ecdf) / length(ecdf_data$ecdf))
  
  plot(ecdf_data$sorted,ecdf_data$ecdf , col="lightgreen",type = "s", lwd=2, main = title)
  
  upper_bound <- ecdf_data$ecdf + 1.96 * z
  lower_bound <- ecdf_data$ecdf - 1.96 * z
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  lines(ecdf_data$sorted, upper_bound, lty = "dashed", col="salmon", lwd=2)
  lines(ecdf_data$sorted, lower_bound, lty = "dashed", col="salmon", lwd=2)
}

ecdf.pwbands.plot(good.wine$pH, "Good Wine pH ECDF and Pointwise Confidence Intervals")
ecdf.pwbands.plot(bad.wine$pH, "Bad Wine pH ECDF and Pointwise Confidence Intervals")
ecdf.pwbands.plot(white.wine.quality$pH, "All Wine pH ECDF and Pointwise Confidence Intervals")

```

4- Plot the empirical distribution functions Fn for all three datasets together with the uniform
confidence bands for α = 0.05. Compare to the bands obtained in (3).

```{r ecdf.ksbands}

ecdf.ksbands <- function(data) {
  ecdf_data <- ecdf.func(data)
  
  D <- 1.3581 / sqrt(length(data))
  upper_bound <- ecdf_data$ecdf + D
  lower_bound <- ecdf_data$ecdf - D
  upper_bound <- ifelse(upper_bound > 1, 1, upper_bound)
  lower_bound <- ifelse(lower_bound < 0, 0, lower_bound)
  
  return(list(sorted = ecdf_data$sorted, ecdf = ecdf_data$ecdf, upper_bound = upper_bound, lower_bound = lower_bound))
}


ecdf.good.ph.ks <- ecdf.ksbands(good.wine$pH)

plot(ecdf.good.ph.ks$sorted, ecdf.good.ph.ks$ecdf,type = "s", col="lightgreen", lwd=2, main = "Good Wine pH ECDF and Uniform Confidence Intervals")
lines(ecdf.good.ph.ks$sorted, ecdf.good.ph.ks$upper_bound, col="salmon", lwd=2, lty = "dashed")
lines(ecdf.good.ph.ks$sorted, ecdf.good.ph.ks$lower_bound, col="salmon", lwd=2, lty = "dashed")
  
ecdf.bad.ph.ks <- ecdf.ksbands(bad.wine$pH)

plot(ecdf.bad.ph.ks$sorted, ecdf.bad.ph.ks$ecdf,type = "s", col="lightgreen", lwd=2, main = "Bad Wine pH ECDF and Uniform Confidence Intervals")
lines(ecdf.bad.ph.ks$sorted, ecdf.bad.ph.ks$upper_bound, col="salmon", lwd=2, lty = "dashed")
lines(ecdf.bad.ph.ks$sorted, ecdf.bad.ph.ks$lower_bound, col="salmon", lwd=2, lty = "dashed")
  
ecdf.all.ph.ks <- ecdf.ksbands(white.wine.quality$pH)

plot(ecdf.all.ph.ks$sorted, ecdf.all.ph.ks$ecdf, type = "s", col="lightgreen", lwd=2, main = "All Wine pH ECDF and Uniform Confidence Intervals")
lines(ecdf.all.ph.ks$sorted, ecdf.all.ph.ks$upper_bound, col="salmon", lwd=2, lty = "dashed")
lines(ecdf.all.ph.ks$sorted, ecdf.all.ph.ks$lower_bound, col="salmon", lwd=2, lty = "dashed")
  
```

Kolmogorov-Smirnov bandwidth for sample size over 35 with 0.05 alpha value is [1.36/sqrt(N)](https://real-statistics.com/statistics-tables/kolmogorov-smirnov-table/) = 0.02382667. The problem with pointwise confidence intervals is, that if we compute them for a lot of points, we accumulate errors. Uniform confidence bands are slightly wider. 

5- Plot the empirical distribution functions of pH for good and bad wines together with the uniform confidence bands in one plot. What can you conclude from this plot?

```{r ecdf.ksbands.ph}
# Create an empty plot with x-axis limits covering the range of both datasets

good_ph <- ecdf.ksbands(good.wine$pH)
bad_ph <- ecdf.ksbands(bad.wine$pH)

# Plot ECDFs for both good and bad wine pH in a single plot
plot(good_ph$sorted, good_ph$ecdf, type="s", col="darkgreen", lwd=2, 
     main="Wine pH ECDF and Uniform Confidence Intervals", xlim = range(c(good.wine$pH, bad.wine$pH)), ylim = c(0, 1), ylab = "ECDF", xlab = "pH")

lines(bad_ph$sorted, bad_ph$ecdf, col="darkred", lwd=2, type="s")

# Add confidence intervals for good wine
lines(good_ph$sorted, good_ph$upper_bound, col="blue", lwd=2, lty = "dashed")
lines(good_ph$sorted, good_ph$lower_bound, col="blue", lwd=2, lty = "dashed")

# Add confidence intervals for bad wine
lines(bad_ph$sorted, bad_ph$upper_bound, col="orange", lwd=2, lty = "dashed")
lines(bad_ph$sorted, bad_ph$lower_bound, col="orange", lwd=2, lty = "dashed")

# Add a legend
legend("bottomright", legend=c("Good Wine (ECDF)", "Bad Wine (ECDF)", "Good Wine (Confidence Intervals)", "Bad Wine (Confidence Intervals)"), 
       col=c("darkgreen", "darkred", "blue", "orange"), lwd=2, cex=0.8, lty=c("solid", "solid", "dashed", "dashed"))
```

On the ECDF we can see that up to a certain point, about to the pH value 3.1, good and bad wine pHs follow a similar distribution. Afterwards the gap increases as there are less bad wines with higher pH values. That is the pH of good wines is higher. Additionally, bad wines seem to have a larger confidence bands on tails compared to good wines.

## Maximum likelihood Estimation
Let X be Laplace distributed with parameters (µ, σ)
t ∈ R × (0, ∞), i.e., X has density

\[ f(x | \mu, σ) = \frac{1}{2σ} \exp\left(-\frac{|x - \mu|}{σ}\right) \]

#### Exercises
1- Consider a sample of independent observations (X1, . . . , Xn). Show that the maximum likelihood estimator for µ is the median of the sample. Is it unique?


Check out this informative [blog](https://math.stackexchange.com/questions/240496/finding-the-maximum-likelihood-estimator
).

Consider the likelihood function for N data samples:
\[L(\mu,\sigma;x) = \prod_{t=1}^{N} \frac{1}{2\sigma} e^{-\frac{|x_t-\mu|}{\sigma}} = (2\sigma)^{-N} e^{-\frac{1}{\sigma}\sum_{t=1}^{N}|x_t-\mu|}\]

Take the log likelihood function:

\[l(\mu,\sigma;x) = -N \ln(2\sigma) - \frac{{1}}{\sigma} \sum_{t=1}^{N}|x_t-\mu|\]


Take the derivative with respect to the parameter $\mu$:
\[ 
\frac{\partial l}{\partial \mu} = -\frac{1}{\sigma} \sum_{t=1}^{N} \frac{\partial |x_t-\mu|}{\partial \mu}
\]

which is equal to:
\[
\frac{1}{\sigma} \sum_{t=1}^{N} \text{sgn}(x_t-\mu) = 0 \quad (1)
\]

The sign function, usually denoted as $\text{sgn}(x)$, returns:
\[
\text{sgn}(x) = 
\begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x = 0 \\
-1 & \text{if } x < 0 
\end{cases}
\]

The median is a solution for both cases when N is an odd or even number. However, it is not unique for the even case. If N is even, {\mu} can take values strictly between $> x_{\left(\frac{n}{2}\right)}$ and $< x_{\left(\frac{n}{2}+1\right)}$. If it is odd it is unique and is equal to the median.

2- Generate n = 20 independent realisations of X with µ = 1 and σ = 1. Determine the maximum likelihood estimator of µ based on this sample using R function quantile. Function quantile has 9 types of sample quantiles. Experiment with the different types of quantiles that are most suitable for the data (justify). Are there any differences? Increase now the sample to n = 1000 and compare different median estimators. Comment on the result.

```{r laplace.quantile}
set.seed(123)

# Set parameters
mu <- 1
sigma <- 1

# Generate 20 independent realizations
n <- 20
laplace.data.20 <- rlaplace(n, mu, sigma)

for (i in 1:9) {
  cat("20 Samples Type ", i, " Median: ", quantile(laplace.data.20, 0.5, type = i), "\n")
}


n <- 1000
laplace.data.1000 <- rlaplace(n, mu, sigma)

for (i in 1:9) {
  cat("1000 Samples Type ", i, " Median: ", quantile(laplace.data.1000, 0.5, type = i), "\n")
}

```

To generate 20 independent realizations of a Laplace distribution with parameters µ = 1 and σ = 1 in R, you can use the rlaplace() function from the jmuOutlier package. They result in the same pattern. Type 1,3 and 4 are the same and the rest are another value. The reason for this is that for types 5 through 9, a different version of linear interpolation is applied. Type 4 is the linear interpolation of the empirical cdf. Type 2 averages at discontinuities and since we have even sample sizes and we are interested in the median, they all result in the same value. Therefore, type 1,3 and 4 rounds down and the rest takes the average of the 10th and 11th or the 500th and the 501st. In our case, type 2,5,6,7,8 and 9 can be used for an unbiased estimator.
When comparing the results for n = 20 and n = 1000, you'll notice that the medians are very close. This is expected because as the sample size increases, the sample median tends to converge to the population median, resulting in more consistent estimates regardless of the quantile type used.

3- Write your own function that calculates the maximum likelihood estimator for a Laplace sample numerically using R function optimise. Describe how R function optimise finds
the maximum. Can you employ a Newton-Raphson algorithm for this problem? Generate n = 20 and n = 1000 independent realisations of X with µ = 1 and σ = 1. Calculate the maximum likelihood estimators based on both samples with your function and using quantile. Compare both estimators, comment on the results.

```{r laplace}

laplace_data <- laplace.data.1000
  laplace.log.likelihood <- function(mu) {
    (-length(laplace_data)*log(2 *sigma) -(1/sigma) * sum(abs(laplace_data - mu)))
  }

optimise(laplace.log.likelihood,maximum = T,
         lower = 0, upper = 10)

laplace_data <- laplace.data.20
optimise(laplace.log.likelihood,maximum = T,
         lower = 0, upper = 10)

```

According to the [documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optimize), optimise() method uses somewhat a combination of golden section search and successive parabolic interpolation. The algorithm starts with an initial interval and evaluates the function at two points within this interval. The points are chosen according to the golden section ratio $φ = (1 + √5) / 2$, which ensures that the interval contracts efficiently.  After the initial evaluations, the algorithm performs successive parabolic interpolations to refine the location of the maximum. This technique utilizes information from previous function evaluations to approximate the maximum more accurately. We get very close optimum results with the corresponding data.

Newton-Raphson method:
\[x_{k+1} = x_k - Q_k\]
\[Q_k = \frac{f'(x_k)}{f''(x_k)}\]

The problem with our function here is that, when we want to maximize the log likelihood, the second derivative of the function is not defined at the parameter µ and is 0 at other values. This makes the update step infinitely large so it is not possible to use Newton's Method here.

optimise() function performs better in both sample sizes because the result is closer to 1. 

4-) Let us now study the distribution of the maximum likelihood estimator. For this, calculate M = 5000 maximum likelihood estimators of µ based on the sample of n = 20 random variables generated from the Laplace distribution with µ = 1 and σ = 1. Repeat the same for the sample size n = 1000. Use histograms and QQ-plots to check if both Monte Carlo samples follow a normal distribution. Compare variances of both distributions, comment on the results.

```{r}
# Function to find MLE for mu using optimise
find_mle_laplace <- function(x) {
  result <- optimise(function(mu) laplace.log.likelihood(mu), maximum = TRUE, lower = -10, upper = 10)
  return(result$maximum)
}


# Parameters
mu <- 1
sigma <- 1
n <- 20
M <- 5000

# Storage for results
results_optimise <- numeric(M)
results_quantile <- numeric(M)

for (i in 1:M) {
  laplace_data <- rlaplace(n, mu, sigma)
  
  # Quantile (median)
  quantile_result <- quantile(laplace_data, 0.5, type = 5)
  results_quantile[i] <- quantile_result
  
  # MLE using optimise
  optimise_result <- find_mle_laplace(laplace_data)
  results_optimise[i] <- optimise_result
  
}

# Display results
cat("First 5 results using optimise:\n", head(results_optimise, 5), "\n")
cat("First 5 results using quantile:\n", head(results_quantile, 5), "\n")

# Plot histograms and QQ-plots
par(mfrow = c(2, 2))
hist(results_optimise, main = "Histogram of MLEs (optimise) for n = 20", xlab = "MLE of mu", breaks = 50)
qqnorm(results_optimise, main = "QQ-plot of MLEs (optimise) for n = 20")
qqline(results_optimise)

hist(results_quantile, main = "Histogram of MLEs (quantile) for n = 20", xlab = "MLE of mu", breaks = 50)
qqnorm(results_quantile, main = "QQ-plot of MLEs (quantile) for n = 20")
qqline(results_quantile)

# Variance comparison
variance_optimise <- var(results_optimise)
variance_quantile <- var(results_quantile)

cat("Variance of MLEs using optimise for n = 20:", variance_optimise, "\n")
cat("Variance of MLEs using quantile for n = 20:", variance_quantile, "\n")

# n 1000
n = 1000

for (i in 1:M) {
  laplace_data <- rlaplace(n, mu, sigma)
  
  # Quantile (median)
  quantile_result <- quantile(laplace_data, 0.5, type = 5)
  results_quantile[i] <- quantile_result
  
  # MLE using optimise
  optimise_result <- find_mle_laplace(laplace_data)
  results_optimise[i] <- optimise_result

}

# Display results
cat("First 5 results using optimise:\n", head(results_optimise, 5), "\n")
cat("First 5 results using quantile:\n", head(results_quantile, 5), "\n")

# Plot histograms and QQ-plots
par(mfrow = c(2, 2))
hist(results_optimise, main = "Histogram of MLEs (optimise) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_optimise, main = "QQ-plot of MLEs (optimise) for n = 1000")
qqline(results_optimise)

hist(results_quantile, main = "Histogram of MLEs (quantile) for n = 1000", xlab = "MLE of mu", breaks = 50)
qqnorm(results_quantile, main = "QQ-plot of MLEs (quantile) for n = 1000")
qqline(results_quantile)


# Variance comparison
variance_optimise <- var(results_optimise)
variance_quantile <- var(results_quantile)

cat("Variance of MLEs using optimise for n = 1000:", variance_optimise, "\n")
cat("Variance of MLEs using quantile for n = 1000:", variance_quantile, "\n")
```
The variances are very close to each other, showing that both methods provide similar variability in the estimates for the larger sample size. As the sample size increases, the distribution of the MLEs tends to approximate the normal distribution more closely, which is consistent with the Central Limit Theorem. The variance of the MLEs decreases significantly as the sample size increases from 20 to 1000. This is expected since larger sample sizes provide more information about the population parameter, leading to more precise estimates.


## Linear Regression

#### Dataset
Go to Kaggle.com and download the data on house prices. This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. There are altogether 19 variables, but we will consider in the analysis only the following ones:

-   price: Price
-   bedrooms: Number of bedrooms
-   bathrooms: Number of bathrooms per bedroom
-   sqft_living: Square footage of the home
-   floors: Total floors in house
-   view: Has been viewed (1 for viewed; 0 for not viewed)
-   condition: How good is the condition (from 1 to 5)
-   grade: Grade given to the housing unit based on King County grading system (from 1 to 13)
-   yr_built: Year the house was built

```{r house.price}
kc_house_data <- read.csv("archive/kc_house_data.csv")

kc_house_data <- kc_house_data %>%
  select(price, bedrooms, bathrooms,
         sqft_living,
         floors,
         view,
         condition,
         grade,
         yr_built)

head(kc_house_data)
```

#### Exercises

1-) Estimate a linear model with the response variable price and all remaining variables as covariates. Are all variables significant? How large is R2 and how can this be interpreted?
Perform the residual analysis to validate the model. Are there any departures from the linear regression model assumptions?

```{r lm}
model <- lm(price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = kc_house_data)

summary(model)
```

The estimated effect of each predictor on the response variable (price). For instance, each additional bedroom decreases the price by approximately 40,650 dollars, while each additional bathroom increases it by about 47,690 dollars. Pr(>|t|): The p-value associated with the t-statistic, testing the null hypothesis that the coefficient is equal to zero (no effect). A lower p-value (typically < 0.05) indicates that the predictor is statistically significant. All variables here are highly significant (p-values < 2e-16), suggesting strong evidence against the null hypothesis. 
R-squared, the proportion of variance in the response variable explained by the predictors. Here, 63.59% of the variance in price is explained by the model. Adjusted R-squared is adjusted for the number of predictors in the model, providing a more accurate measure when multiple predictors are used.
The summary indicates a strong model fit, with all predictors being statistically significant and the model explaining a substantial portion of the variance in house prices. However, the relatively high residual standard error suggests that while the model captures a significant portion of the variability, there is still considerable unexplained variation in house prices.

```{r lm.plot}
par(mfrow = c(2, 2))
plot(model)
```
On the Residuals vs Fitted plot, homoscedasticity of ordinary least squares estimation is clearly violated. Variance in residuals increases as fitted values increase. We can additionally support the violation of homoscedasticity by looking at the crookedness of Scale-Location plot. Q-Q plot of residuals is violated too. There are strong lower and upper tails. On the Residuals vs Leverage plot, even though there are points that are close to boundaries, most of them are within the range. So we can say that there are not many influential or outlier cases in linear regression analysis.

2-) Produce a histogram and a QQ-plot of the response variable price, as well as of its log transform log(price). Compare both distributions to the normal one. Fit now a linear model with the response variable log(price). Compare the estimated model with the one from (a) in terms of R2, significance and effect of covariates and model fit (via residual analysis). Which model is more adequate?

```{r plot.log}

kc_house_data$log_price <- log(kc_house_data$price)

par(mfrow = c(2, 2))
# Histogram of price
hist(kc_house_data$price, main = "Histogram of Price", xlab = "Price", breaks = 30)

# QQ-plot of price
qqnorm(kc_house_data$price, main = "QQ-Plot of Price")
qqline(kc_house_data$price)

# Histogram of log(price)
hist(kc_house_data$log_price, main = "Histogram of Log(Price)", xlab = "Log(Price)", breaks = 30)

# QQ-plot of log(price)
qqnorm(kc_house_data$log_price, main = "QQ-Plot of Log(Price)")
qqline(kc_house_data$log_price)

```

Both the Histogram and QQ-Plot of price shows a very high right tail. Log(price) still has a slightly heavy tail but it is considerably very low.

```{r lm.log}
# Fit the linear model with log(price) as response
model_log_price <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = kc_house_data)

# Summary of the model
summary(model_log_price)
```

The R-squared values are slightly higher in the log model, indicating that this model explains a bit more variance in the response variable compared to the original model. In both models, all predictors are statistically significant with p-values < 2e-16. The t-values for the predictors are generally higher in the log model, suggesting stronger relationships between predictors and the log-transformed response variable. In the log transformed models, the interpretation of coefficients is slightly different. Following the bathroom independent variable, each unit of increase now means, the dependent variable is multiplied by $e^(169.3)$.

```{r lm.log.plot}
par(mfrow = c(2, 2))
plot(model_log_price)
```

Residuals vs Fitted and Scale-Location plots shows that homoscedasticity problem is highly resolved. Q-Q Plot also follows a straight line. And we cannot even see the most of dashed lines in Residuals vs Leverage. The point 12778 seems pretty close to the one below tho.

In summary, the model with log(price) as the response variable appears to be more appropriate as it:

-   Has a higher R-squared value, indicating a better fit.
-   Shows stronger relationships between predictors and the response variable.
-   Demonstrates improved residual distribution, suggesting fewer outliers and a more consistent error structure.

3-) In the model from (b) interpret the effect of each covariate on the response. Plot each covariate against log(price). Is the assumption of the linear dependence between covariates and
response plausible for all covariates? Add to the model from (b) squared terms for yr_built
and sqft_living. Are these terms significant? Does adding these two terms improve the
model fit in terms of R2?

```{r covariates, warning=FALSE, message=FALSE}
covariates <- c("bedrooms", "bathrooms", "sqft_living", "floors", "view", "condition", "grade", "yr_built")

# Create plots
for (var in covariates) {
  p <- ggplot(kc_house_data, aes_string(x = var, y = "log_price")) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", col = "blue") +
    labs(title = paste("Log(price) vs", var), x = var, y = "Log(price)")
  print(p)
}
```

Variables bathrooms and grade can likely have a linear relationship. But the rest of variables seem to either have a different relationship such as sqft_living or they don't explain any variance in the predictor variables such as yr_built.

```{r squared.lm}
# Fit the new model with squared terms
model_log_price_sq <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built + I(yr_built^2) + I(sqft_living^2), data = kc_house_data)

summary(model_log_price_sq)
```

The terms are significant. And the R2 value is slightly improved from 0.6426 to 0.6492. Adjusted R-Squared is increased as well.

4-) Now we would like to compare how well models from (b) and (c) make prediction. For this divide the dataset into a training and a test set. Sample randomly 10 806 rows to include into the training set and the rest will be the test set. To ensure comparability of the results set.seed(1122) before sampling. Fit both models on the training set and make prediction on the test set. Calculate the mean squared difference between predicted values and values of log(price) from the test set for each model. Which prediction error is smaller? Try to extend the model to improve the prediction: my best model gives prediction error of 0.09557445.

```{r mse.extention}
set.seed(1122)
training_indices <- sample(seq_len(nrow(kc_house_data)), size = 10806)
training_set <- kc_house_data[training_indices, ]
test_set <- kc_house_data[-training_indices, ]

model_log_price_train <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = training_set)

model_log_price_sq_train <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built + I(yr_built^2) + I(sqft_living^2), data = training_set)

pred_log_price <- predict(model_log_price_train, newdata = test_set)

pred_log_price_sq <- predict(model_log_price_sq_train, newdata = test_set)

mse_log_price <- mean((test_set$log_price - pred_log_price)^2)
mse_log_price

mse_log_price_sq <- mean((test_set$log_price - pred_log_price_sq)^2)
mse_log_price_sq

summary(model_log_price_sq_train)

# Extention

model_log_price_new <- lm(log_price ~ bedrooms +  bedrooms : bathrooms  +  bathrooms : grade + sqft_living +sqft_living:floors + grade + floors  + condition + yr_built + I(yr_built^2) + I(sqft_living^2) +view + view:bathrooms  + I(floors^2) +I(grade^2), data = training_set)

pred_log_price_new <- predict(model_log_price_new, newdata = test_set)

mse_log_price_new <- mean((test_set$log_price - pred_log_price_new)^2)

print(paste0("Prediction error: ", mse_log_price_new))
summary(model_log_price_new)

```

Although the model model_log_price_new  or pred_log_price_sq performs slightly better, it usually does not mean the model generalizes better. I particularly had to try many different interaction effects or polynomial fits on this split. With different splits, different interactions or effects can give different results.


## Penalized Regression

#### Dataset

Consider the dataset hitters.dat on baseball players that has been analysed in the book An introduction to statistical learning by G. James, D. Witten, T. Hastie and R. Tibshirani (2013, Springer). It contains the following 20 variables:

-   AtBat: Number of times at bat in 1986
-   Hits: Number of hits in 1986
-   HmRun: Number of home runs in 1986
-   Runs: Number of runs in 1986
-   RBI: Number of runs batted in 1986
-   Walks: Number of walks in 1986
-   Years: Number of years in the major leagues
-   CAtBat: Number of times at bat during his career
-   CHits: Number of hits during his career
-   CHmRun: Number of home runs during his career
-   CRuns: Number of runs during his career
-   CRBI: Number of runs batted in during his career
-   CWalks: Number of walks during his career
-   League: A factor with levels A and N indicating player’s league at the end of 1986
-   Division: A factor with levels E and W indicating player’s division at the end of 1986
-   PutOuts: Number of put outs in 1986
-   Assists: Number of assists in 1986
-   Errors: Number of errors in 1986
-   Salary: 1987 annual salary on opening day in thousands of dollars
-   NewLeague: A factor with levels A and N indicating player’s league at the beginning of 1987

We would like to explain Salary of a player using the other variables. The function glmnet in the package glmnet is used to fit both ridge and lasso regression. Supply α = 0 for ridge and α = 1 for
lasso when calling glmnet. Make sure you understand the interpretation of λ in both cases (note that it is not the same for lasso and ridge!). Read the documentation ?glmnet to understand what
is the exact form of the objective function that is being minimised.


1-) Load the dataset into R and create a new dataset containing only those players for which all
data is available.

```{r hitters.df}
Hitters <- Hitters[complete.cases(Hitters),]

```

2-) Find the condition number (ratio between largest and smallest eigenvalue) of $X^T X$, where y is the salary and X represents all the other variables. What can you say about the condition number? Does it help if you standardise the design matrix, such that its columns (without the intercept) have mean zero and variance one?

```{r condition.number.hitters}

x = model.matrix(Salary~., Hitters)[,-1]
condition.number <- function (x){
  z <- t(x) %*% x
  print(max(svd(z)$d)/min(svd(z)$d))
  print(kappa(z, exact = T))
  }

condition.number(x)
condition.number(scale(x))

```

The condition number is a good indicator of how close is a matrix to be singular. The larger the condition number the closer we are to singularity. A matrix with large condition number is nearly singular, whereas a matrix with a condition number close to 1 is far from being singular.
Let $\boldsymbol{x}$ be the solution of {\bf A} \boldsymbol{x} = \boldsymbol{b}
 be the solution of the perturbed problem ${\bf A} \hat{\boldsymbol{x}} = \boldsymbol{b} + \Delta \boldsymbol{b}$ . Let $\Delta \boldsymbol{x} = \hat{\boldsymbol{x}} - \boldsymbol{x}$ be the absolute error in output. Then we have ${\bf A} \boldsymbol{x} + {\bf A} \Delta \boldsymbol{x} = \boldsymbol{b} + \Delta \boldsymbol{b}$, so ${\bf A} \Delta \boldsymbol{x} = \Delta \boldsymbol{b}.$ Now we want to see how the relative error in output $\left(\frac{\|\Delta \boldsymbol{x}\|}{\|\boldsymbol{x}\|}\right)$ is related to the relative error in input $\left(\frac{\|\Delta \boldsymbol{b}\|}{\|\boldsymbol{b}\|}\right)$

\[
\begin{align}
\frac{\|\Delta \boldsymbol{x}\| / \|\boldsymbol{x}\|}{\|\Delta \boldsymbol{b}\| / \|\boldsymbol{b}\|} &= \frac{\|\Delta \boldsymbol{x}\| \|\boldsymbol{b}\|}{\|\boldsymbol{x}\| \|\Delta \boldsymbol{b}\|}\\
&= \frac{\|{\bf A}^{-1} \Delta \boldsymbol{b}\| \|{\bf A} \boldsymbol{x}\|}{\|\boldsymbol{x}\| \|\Delta \boldsymbol{b}\|}\\
&\le \frac{\|{\bf A}^{-1}\| \|\Delta \boldsymbol{b}\| \|{\bf A}\| \|\boldsymbol{x}\|}{\|\boldsymbol{x}\| \|\Delta \boldsymbol{b}\|} \\
&= \|{\bf A}^{-1}\| \|{\bf A}\|\\ &= \text{cond}({\bf A})
\end{align}\]

where we used $\|{\bf A}\boldsymbol{x}\| \le \|{\bf A}\| \|\boldsymbol{x}\|, \forall \boldsymbol{x}$.
Then
\[
\frac{\|\Delta \boldsymbol{x}\|}{\|\boldsymbol{x}\|} \le \text{cond}({\bf A})\frac{\|\Delta \boldsymbol{b}\|}{\|\boldsymbol{b}\|}  \qquad (1)
\]

Therefore, if we know the relative error in input, then we can use the condition number of the system to obtain an upper bound for the relative error of our computed solution (output). For more information, check out the [link](https://courses.engr.illinois.edu/cs357/sp2024/notes/ref-10-condition.html).


When the matrix is standardized, condition number decreases drastically. In this [answer](https://stats.stackexchange.com/questions/243000/cause-of-a-high-condition-number-in-a-python-statsmodels-regression#comment462188_243000) the reason for a high condition number is clearly explained. This is because they're fitting a line to the points and then projecting the line all the way back to the origin (x=0) to find the y-intercept. That y-intercept will be very sensitive to small movements in the data points. The condition number takes into account high sensitivity in either fitted parameter to the input data, hence the high condition number when all of the data are far to one side of x=0.

 
3-) Fit a standard linear model (no regularisation) and a ridge regression with λ = 70. Compare the size of the coefficients in the two models. What do you observe?

```{r lm.hitters}
y = Hitters %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

# Fit a standard linear model
lm_model <- lm(Salary ~ ., data = Hitters)

# Get the coefficients of the linear model
lm_coefficients <- coef(lm_model)
print("Linear Model Coefficients:")
print(lm_coefficients)


# Fit a ridge regression model with lambda = 70
ridge_model <- glmnet(x, y, alpha = 0, lambda = 70)

# Get the coefficients of the ridge regression model
ridge_coefficients <- coef(ridge_model)
print("Ridge Regression Coefficients with lambda = 70:")
print(ridge_coefficients)



# Standard Linear Model Coefficients
lm_coefficients <- coef(lm_model)
lm_coefficients <- as.data.frame(lm_coefficients)
colnames(lm_coefficients) <- "LinearModel"

# Ridge Regression Coefficients with lambda = 70
ridge_coefficients <- coef(ridge_model)
ridge_coefficients <- as.data.frame(as.matrix(ridge_coefficients))
colnames(ridge_coefficients) <- "RidgeRegression"

# Combine the data frames, making sure row names are kept as a column
comparison_df <- data.frame(
  Variable = rownames(lm_coefficients),
  LinearModel = lm_coefficients$LinearModel,
  RidgeRegression = ridge_coefficients$RidgeRegression
)

comparison_df_tidy <- pivot_longer(
  comparison_df,
  cols = c("LinearModel", "RidgeRegression"),
  names_to = "Model",
  values_to = "Coefficient"
)


ggplot(comparison_df_tidy, aes(x = Variable, y = Coefficient, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Comparison of Coefficients: Linear Model vs. Ridge Regression",
       x = "Variable",
       y = "Coefficient Value")

```

glm() by default standardizes the data. The importance of standardization in regularized models are thoroughly examined in the exercise before. Here in Ridge Regression, the coefficients are generally smaller, indicating that the regularization term λ is effectively shrinking them. This often results in a model that generalizes better to new data by mitigating multicollinearity and reducing overfitting.

4-) The value 70 for λ is arbitrary and we would like to find a data-driven way to choose it. The criterion to compare is the mean squared prediction error as in exercise 4 (d) on linear regression. Split the data randomly into a training and a test set with set.seed(1122).

```{r empty}

set.seed(1122)

train = Hitters %>%
  sample_frac(0.8)

test = Hitters %>%
  setdiff(train)

x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]

y_train = train %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

y_test = test %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam

plot(cv.out)

# What is the test MSE associated with this value of  λ?
ridge_model.best.l <- glmnet(x_train, y_train, alpha=0, lambda = bestlam)
ridge_pred = predict(ridge_model.best.l, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((ridge_pred - y_test)^2)
```

I think the best way to perform this task would be to use the built-in cross-validation function, cv.glmnet(). By default, the function performs 10-fold cross-validation.

5-) Write a function that takes λ as argument, fits a ridge regression on the training sets and calculates the mean squared prediction error on the test set. Run this function on a logarithmic grid (e.g., 10^seq(from = 10, to = -2, length = 100)). Plot the results against log(λ) and graphically find the value λopt that minimises the mean squared prediction error.

```{r}
ridge_mse <- function(lambda, x_train, y_train, x_test, y_test) {
  ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = lambda)
  y_pred <- predict(ridge_mod, s = lambda, newx = x_test)
  mse <- mean((y_test - y_pred)^2)
  return(mse)
}

# Grid of lambda values
lambda_grid <- 10^seq(10, -2, length = 100)

# Calculate MSE for each lambda
mse_values <- sapply(lambda_grid, ridge_mse, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test)

# Find the optimal lambda that minimizes MSE
optimal_lambda <- lambda_grid[which.min(mse_values)]
optimal_mse <- min(mse_values)

# Plot the results
plot_data <- data.frame(
  log_lambda = log(lambda_grid),
  mse = mse_values
)

ggplot(plot_data, aes(x = log_lambda, y = mse)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = log(optimal_lambda), linetype = "dashed", color = "red") +
  annotate("text", x = log(optimal_lambda), y = optimal_mse, label = paste0("lambda: ", round(optimal_lambda, 2), "\nMSE: ", round(optimal_mse, 2)), hjust = -0.1, vjust = -1.5, color = "red") +
  labs(title = "MSE vs. log(lambda)",
       x = "log(lambda)",
       y = "Mean Squared Error") +
  theme_minimal()

# Print the optimal lambda value
cat("The optimal lambda value is:", optimal_lambda, "\n")

```

6-)  Fit a ridge regression with λopt on all the data, and interpret some of the coefficients. Which are the most important variables? Are there coefficients that equal zero exactly?

```{r}

# Fit a ridge regression with optimal lambda on all the data
ridge_model_optimal <- glmnet(x, y, alpha = 0, lambda = optimal_lambda)
optimal_ridge_coefficients <- as.vector(coef(ridge_model_optimal))
names(optimal_ridge_coefficients) <- rownames(coef(ridge_model_optimal))

print("Ridge Regression Coefficients with optimal lambda:")

important_variables <- sort(abs(optimal_ridge_coefficients), decreasing = TRUE)
important_variables


```
Intercept and DivisionW are pretty important in ridge regression. There are not any coefficients that are set to 0.


7-) Repeat parts (d), (e) and (f) for lasso instead of ridge. Are there now coefficients that are equal to zero?


```{r}

# Fit lasso regression model with cross-validation
cv.out_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
bestlam_lasso <- cv.out_lasso$lambda.min

# Plot cross-validation results
plot(cv.out_lasso)

# Calculate the test MSE associated with the best lambda for lasso
lasso_model_best <- glmnet(x_train, y_train, alpha = 1, lambda = bestlam_lasso)
lasso_pred <- predict(lasso_model_best, s = bestlam_lasso, newx = x_test)
test_mse_lasso <- mean((lasso_pred - y_test)^2)
cat("The test MSE associated with the best CV lambda for lasso is:", test_mse_lasso, "\n")

# Define the function to calculate MSE
lasso_mse <- function(lambda, x_train, y_train, x_test, y_test) {
  lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = lambda)
  y_pred <- predict(lasso_mod, s = lambda, newx = x_test)
  mse <- mean((y_test - y_pred)^2)
  return(mse)
}

# Grid of lambda values
lambda_grid_lasso <- 10^seq(10, -2, length = 100)

# Calculate MSE for each lambda
mse_values_lasso <- sapply(lambda_grid_lasso, lasso_mse, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test)

# Find the optimal lambda that minimizes MSE
optimal_lambda_lasso <- lambda_grid_lasso[which.min(mse_values_lasso)]
optimal_mse_lasso <- min(mse_values_lasso)

# Plot the results
plot_data_lasso <- data.frame(
  log_lambda = log(lambda_grid_lasso),
  mse = mse_values_lasso
)

ggplot(plot_data_lasso, aes(x = log_lambda, y = mse)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = log(optimal_lambda_lasso), linetype = "dashed", color = "red") +
  annotate("text", x = log(optimal_lambda_lasso), y = optimal_mse_lasso, label = paste0("lambda: ", round(optimal_lambda_lasso, 2), "\nMSE: ", round(optimal_mse_lasso, 2)), hjust = -0.1, vjust = -1.5, color = "red") +
  labs(title = "MSE vs. log(lambda) for Lasso",
       x = "log(lambda)",
       y = "Mean Squared Error") +
  theme_minimal()

cat("The optimal lambda value for lasso is:", optimal_lambda_lasso, "\n")


# Fit a lasso regression with optimal lambda on all the data
lasso_model_optimal <- glmnet(x, y, alpha = 1, lambda = optimal_lambda_lasso)
optimal_lasso_coefficients <- as.vector(coef(lasso_model_optimal))
names(optimal_lasso_coefficients) <- rownames(coef(lasso_model_optimal))

important_lasso_variables <- sort(abs(optimal_lasso_coefficients), decreasing = TRUE)
important_lasso_variables


```

The difference between ridge and lasso regression is that Lasso tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero. Therefore, Lasso puts more weights on individual coefficients. Intercept, Hits, Walks, CRBI and CRuns are all non zero in Lasso regression. 


## Logistic regression

#### Dataset
Go to UCI Machine learning repository and download the data on blood donation. This page contains also the background information on the data. The goal is to build a model that allows to
predict best if a donor will donate blood. The dataset contains the following variables:

-   recency: months since last donation
-   frequency: total number of donations
-   amount: total blood donated in c.c.
-   time: months since first donation
-   donation: 1 stands for donating blood, 0 stands for not donating blood

#### Exercises

1-) Read the data into R and
fit a generalised linear model with the binary response donation and covariate frequency using the canonical link function. Fit the same model replacing the covariate by amount. Compare it to the first model. Plot the variable frequency against amount. Comment on the results. Do you need both of these variables in the model?


```{r}
# Read the data into R
transfusion <- read.csv("transfusion.data", col.names = c("recency", "frequency", "amount", "time", "donation"))

# Fit a GLM with donation as the response and frequency as the predictor
model_frequency <- glm(donation ~ frequency, data = transfusion, family = binomial(link = "logit"))

# Summary of the model
summary(model_frequency)

# Fit a GLM with donation as the response and amount as the predictor
model_amount <- glm(donation ~ amount, data = transfusion, family = binomial(link = "logit"))

# Summary of the model
summary(model_amount)

# Compare the two models using AIC
aic_frequency <- AIC(model_frequency)
aic_amount <- AIC(model_amount)

cat("AIC for model with frequency as predictor:", aic_frequency, "\n")
cat("AIC for model with amount as predictor:", aic_amount, "\n")

# Plot frequency against amount
plot(transfusion$frequency, transfusion$amount,
     xlab = "Frequency",
     ylab = "Amount",
     main = "Frequency vs Amount",
     pch = 19,
     col = "blue")

# Check correlation between frequency and amount
correlation <- cor(transfusion$frequency, transfusion$amount)
cat("Correlation between frequency and amount:", correlation, "\n")

```

The correlation of 1 between frequency and amount indicates perfect collinearity, meaning that these two variables provide the same information. In this case, including both variables in the model is redundant, and it would be appropriate to use only one of them.

Since the AIC values for the models with frequency and amount as predictors are the same, it confirms that both predictors contribute equally to the model. Given this perfect collinearity, we only need one of these variables in our model.


2-) Fit now the GLM model with the response donation and covariate recency using all link functions available in the glm function. Compare obtained estimators and comment on the
differences.

```{r}
fit_logit <- glm(donation ~ recency, data = transfusion, family = binomial(link = "logit"))
fit_probit <- glm(donation ~ recency, data = transfusion, family = binomial(link = "probit"))
fit_cauchit <- glm(donation ~ recency, data = transfusion, family = binomial(link = "cauchit"))
fit_log <- glm(donation ~ recency, data = transfusion, family = binomial(link = "log"))
fit_cloglog <- glm(donation ~ recency, data = transfusion, family = binomial(link = "cloglog"))


summary(fit_logit)
summary(fit_probit)
summary(fit_cauchit)
summary(fit_log)
summary(fit_cloglog)

```

According to the documentation of ?glm(), "...the binomial family the links logit, probit, cauchit, (corresponding to logistic, normal and Cauchy CDFs respectively) log and cloglog (complementary log-log)".

The comparison of GLM models using different link functions (logit, probit, cauchit, log, and cloglog) reveals distinct differences in their coefficient estimates and model fit. The logit and probit models, which are often similar, both show significant coefficients for recency, with the logit model having slightly larger coefficients. The cauchit model, based on the Cauchy distribution, exhibits more extreme coefficient values and a higher AIC, indicating a less favorable fit compared to logit and probit. The log model provides a different scale for the intercept and slightly smaller coefficients for recency, with an AIC comparable to the logit model.

The complementary log-log (cloglog) model shows the lowest AIC, suggesting the best fit among the models tested. Despite these variations, all models consistently indicate that recency is a significant predictor of donation.

3-) Now we would like to build a model that makes the best prediction for the blood donations.
-   First divide the dataset into a training and a test set. Sample randomly 374 rows to include into the training set and the rest will be the test set. To ensure comparability of the results set.seed(1122) before sampling. Fit a GLM model with the response donation and canonical link on the training set, choosing appropriate covariates. Predict the model
on the test set.

```{r}
set.seed(1122)

# Randomly sample 374 rows for the training set
train_indices <- sample(1:nrow(transfusion), 374)
train_set <- transfusion[train_indices, ]
test_set <- transfusion[-train_indices, ]

# Fit a GLM model with the canonical link (logit) on the training set
glm_model <- glm(donation ~ recency + frequency + time, data = train_set, family = binomial(link = "logit"))

# Predict the model on the test set
test_set$predicted_prob <- predict(glm_model, newdata = test_set, type = "response")

# Print the summary of the fitted model
summary(glm_model)

```

The canonical link function for the binomial distribution in generalized linear models (GLMs) is the *logit* function. 



-   With the predicted probability perform the classification: set the predicted ith value of donation to 0, if the corresponding ith predicted probability is less than 0.5 and to 1 otherwise. Assess the goodness of your classification calculating the mean absolute error.  Try to extend the model to improve the classification error. Can you beat a performance of 0.2085561?

```{r}
train_set$log_amount <- log(train_set$amount + 1)
train_set$amount_t <- train_set$amount / train_set$time
train_set$r_a <- train_set$amount * train_set$recency

test_set$log_amount <- log(test_set$amount + 1)
test_set$amount_t <- test_set$amount / test_set$time
test_set$r_a <- test_set$amount * test_set$recency

glm_model_extended <- glm(donation ~ recency  + time + log_amount + r_a, data = train_set, family = binomial(link = "logit"))

summary(glm_model_extended)

test_set$predicted_prob_extended <- predict(glm_model_extended, newdata = test_set, type = "response")


test_set$predicted_class <- ifelse(test_set$predicted_prob < 0.5, 0, 1)

test_set$predicted_class_extended <- ifelse(test_set$predicted_prob_extended < 0.5, 0, 1)

mean(abs(test_set$donation - test_set$predicted_class))
mean(abs(test_set$donation - test_set$predicted_class_extended))

```


## Generalized linear models


#### Dataset
The dataset student-mat.csv can be found on Kaggle. This page contains a full description of the data and all the variables. Variables G1, G2, and G3 are first, second, and final grades in
mathematics. The remaining variables are explanatory variables. We would like to identify variables that explain grades in mathematics.

1-) First we need to identify the distribution of each of G1, G2, and G3. Can each of these variables be assumed to follow a normal distribution? Justify your answer using suitable arguments and graphical tools. Can each of G1, G2, and G3 be assumed to follow a Poisson distribution?
Are there signs for over-dispersion or any other anomalies in the distributions of any of G1, G2, or G3? Support your answer using suitable arguments and graphical tools.

```{r, warning=FALSE}
student.mat <- read.csv("student-mat.csv")

head(student.mat)

summary(student.mat[, c("G1", "G2", "G3")])


# Normality Assessment
# Plot histograms
hist(student.mat$G1, breaks = 10, main = "Histogram of G1", xlab = "G1")
hist(student.mat$G2, breaks = 10, main = "Histogram of G2", xlab = "G2")
hist(student.mat$G3, breaks = 10, main = "Histogram of G3", xlab = "G3")

# Plot Q-Q plots
qqnorm(student.mat$G1, main = "Q-Q Plot of G1")
qqline(student.mat$G1)
qqnorm(student.mat$G2, main = "Q-Q Plot of G2")
qqline(student.mat$G2)
qqnorm(student.mat$G3, main = "Q-Q Plot of G3")
qqline(student.mat$G3)



dispersion.test.poisson <- function(data) 
{
  obs_freq <- table(data)

  lambda <- mean(data)
  exp_freq <- dpois(as.numeric(names(obs_freq)), lambda) * length(data)
  
  chisq_test <- chisq.test(obs_freq, p = exp_freq, rescale.p = TRUE)
  print(chisq_test)
  
  qqplot(qpois(ppoints(length(data)), lambda), data,
         main = "QQ Plot for Poisson Distribution",
         xlab = "Theoretical Quantiles",
         ylab = "Sample Quantiles")
  abline(0, 1, col = "red")
}


cat("G1 Poisson: ", dispersion.test.poisson(student.mat$G1))
cat("G2 Poisson: ", dispersion.test.poisson(student.mat$G2))
cat("G3 Poisson: ", dispersion.test.poisson(student.mat$G3))

```

Distributions are not normal. They highly deviate from the normal distribution. Looking at the distributions, we can make the assumptions that G1 might be underdispersed, G2 and G3 might be overdispersed looking at the QQ plots. Dispersion parameter above 1 also supports that. We can try a Chi-squared Goodness-of-fit test, A p-value greater than 0.05 typically suggests that the data does not significantly deviate from a Poisson distribution, in hypothesis testing lingo, we cannot conclude that the data do not follow a poisson distribution. Looking at the results G1 seems to be the closest to poisson distribution but it is still not significant.

2-) Fit a suitable (generalised) linear model to explain G1 including all explanatory variables (Model 1). Are all covariates significant? Comment on the goodness-of-fit of this model. Calculate the Pearson residuals and Anscombe residuals and assess how closely they follow a normal distribution. Pursue the residual analysis and comment if the fitted (generalised) linear model is adequate for the data.


```{r}
model_G1 <- glm(G1 ~ .-G2-G3, data = student.mat, family = poisson)
summary(model_G1)

predicted_G1 <- fitted(model_G1)

#  for Poisson regression var(predicted_G1) = predicted_G1
pearson_residuals_G1 <- (student.mat$G1 - predicted_G1) / sqrt(predicted_G1)

anscombe_residuals_G1 <- (3/2) * (student.mat$G1^(2/3) * predicted_G1^(-(1/6)) - predicted_G1^(1/2))

hist(pearson_residuals_G1, main = "Pearson Residuals for G1", xlab = "Pearson Residuals", probability = TRUE)
qqnorm(pearson_residuals_G1, main = "Q-Q Plot of Pearson Residuals for G1")
qqline(pearson_residuals_G1)

hist(anscombe_residuals_G1, main = "Anscombe Residuals for G1", xlab = "Anscombe Residuals", probability = TRUE)
qqnorm(anscombe_residuals_G1, main = "Q-Q Plot of Anscombe Residuals for G1")
qqline(anscombe_residuals_G1)

# Calculate residuals and fitted values for G1
residuals_G1 <- residuals(model_G1)

# Half-normal plot and residuals vs. fitted values plot for G1
par(mfrow = c(1, 2), mar = c(5, 5, 1, 1))

# Half-normal plot of sorted residuals for G1
halfnorm(residuals_G1, ylab = "Sorted Residuals for G1")

# Residuals vs. fitted values plot (log scale) for G1
plot(log(predicted_G1), log((student.mat$G1 - predicted_G1)^2), 
     xlab = expression(hat(y)), ylab = expression((y - hat(y))^2),
     main = "Log Squared Residuals vs. Log Fitted Values")
abline(0, 1)

# Residual analysis anscombe
par(mfrow = c(1, 2), mar = c(5, 5, 1, 1))

# Half-normal plot of sorted Anscombe residuals
halfnorm(anscombe_residuals_G1, ylab = "Sorted Anscombe Residuals for G1")

# Residuals vs. fitted values plot (log scale) for Anscombe residuals
plot(log(predicted_G1), log(anscombe_residuals_G1^2), 
     xlab = expression(hat(y)), ylab = expression((y - hat(y))^2),
     main = "Log Squared Anscombe Residuals vs. Log Fitted Values")
abline(0, 1)

# Anscombe residual analysis
halfnorm(anscombe_residuals_G1, ylab = "Sorted Anscombe Residuals for G1")

plot(log(predicted_G1), log(anscombe_residuals_G1^2), 
     xlab = expression(hat(y)), ylab = expression((y - hat(y))^2),
     main = "Log Squared Anscombe Residuals vs. Log Fitted Values")
abline(0, 1)


# Dispersion
sum(residuals(model_G1,type="pearson")^2)/model_G1$df.res


```

Only the intercept, sexM, studytime, failures, schoolsupyes, famsupyes and goout seem to be significant. Dispersion can be considered as the goodness of fit of the model. As we expected, it is underdispersed with the dispersion value 0.7303499 < 1.

According to the book [2], "A disadvantage of the Pearson residual is that the distribution of pearson residual for non-Normal distributions is often markedly skewed, and so  it may fail to have properties similar to those of a Normal-theory residual. Anscombe proposed defining a residual using a function A(y) in place of y, where A(.) is chosen to make the distribution of A(Y) "as Normal as possible'. When we look at both residuals, they do not follow a normal distribution. They are less dispersed than a normal distribution. 

Residual analysis shows a very similar result. Most of the values are below the line and there are certainly some outliers.



3-) Take Model 1, but reduce the covariates to sex, Fedu, studytime, failures, schoolsup, famsup, goout (Model 2). Are all the covariates significant? Interpret the effect of each covariate on the grade. Assess the goodness-of-fit of this model. Perform analysis of deviance test to compare Model 1 and Model 2. Comment on the results. In Model 2 replace goout by Walc to get Model 3. How one can compare Model 2 and Model 3? Which model delivers a better fit? Justify your answer.


```{r}
compute_deviance <- function(model) {
  y <- model$y
  mu <- fitted(model)
  deviance <- 2 * sum(y * log(y / mu) - (y - mu))
  return(deviance)
}

model2_G1 <- glm(G1 ~ sex + Fedu + studytime + failures + schoolsup + famsup + goout, 
                        data = student.mat, family = poisson)

summary(model2_G1)

deviance_full <- compute_deviance(model_G1)
deviance_m2 <- compute_deviance(model2_G1)

delta_D <- deviance_m2 - deviance_full

df <- length(coef(model_G1)) - length(coef(model2_G1))

# Critical value at 5% significance level
critical.value <- qchisq(0.95, df)

ifelse(delta_D > critical.value, "Reject reduced model, full model is better", "Fail to reject reduced model")

#Model 3
model3_G1 <- glm(G1 ~ sex + Fedu + studytime + failures + schoolsup + famsup + Walc, 
                        data = student.mat, family = poisson)

deviance_m3 <- compute_deviance(model3_G1)

delta_D.2 <- deviance_m3 - deviance_m2
df.2 <- length(coef(model2_G1)) - length(coef(model3_G1))

critical.value.2 <- qchisq(0.95, df.2)

ifelse(delta_D.2 > critical.value.2, "Reject model 3, model 2 is better", "Fail to reject reduced model")

```


All covariates are significant at various levels, indicating they all contribute to the model2_G1 at the significance p < 0.05. A lower deviance and AIC compared to the null deviance (402.47) indicate a good fit of Model 2. In the first comparison, "Fail to reject reduced model" indicates Model 2 is not significantly worse than Model 1; thus, the reduced model is sufficient. In the second one "Reject model 3, model 2 is better" suggests replacing goout with Walc worsens the fit, and Model 2 is preferred.

## Mixed effects models and small area estimation


#### Dataset
Consider the survey and satellite data measuring the area for corn and soy fields in North-Central Iowa from 1978. Information is only available for few segments for the counties of interest. Detailed information was made available by passes of NASA’s LANDSAT satellites. The number of pixels for both crops is given up to segment level. The data set is available as landsat in the R-package JoSAE. We are interested in obtaining reliable estimates for the total size of corn and soy production for each of the 12 counties in the data set, respectively. Variables of interest:

-   SegmentsInCounty: number of of segments of county.
-   SegementID: identificator for segment.
-   HACorn: hectares of corn for given segment.
-   HASoybeans: hectares of soybeans for given segment.
-   PixelsCorn: pixels for corn for given segment.
-   PixelsSoybeans: pixels for soybeans for given segment.
-   MeanPixelsCorn: mean of pixels for corn over all segments in given county.
-   MeanPixelsSoybeans: mean of pixels for soybeans over all segments in given county.
-   CountyName: county identificator of the segment.

```{r}

data(landsat)
landsat <- landsat[, c("SegmentsInCounty"
            ,"SegementID"
            ,"HACorn"
            ,"HASoybeans"
            ,"PixelsCorn"
            ,"PixelsSoybeans"
            ,"MeanPixelsCorn"
            ,"MeanPixelsSoybeans"
            ,"CountyName")]


head(landsat)
```

1-) Fit a suitable linear model to both the hectares of corn and soybeans for segment for each county. Explain your choice of included parameters. What are the limitations of the linear model? You might want to create a groupedData-object and use the nlme-function lmList.


```{r}
corn_data <- groupedData(HACorn ~ PixelsCorn | CountyName, data = landsat)
soy_data <- groupedData(HASoybeans ~ PixelsSoybeans | CountyName, data = landsat)

# Fit linear models for each county
lm_corn <- lmList(HACorn ~ PixelsCorn | CountyName, data = corn_data)
lm_soy <- lmList(HASoybeans ~ PixelsSoybeans | CountyName, data = soy_data)

summary(lm_corn)
summary(lm_soy)
```

Since MeanPixelsCorn and MeanPixelsSoybeans are the same for each country, it does not make sense to include them. Additionally if we included SegementID as groupedData, we would simply overfit to each instance.

In corn model some counties (Worth, Hamilton, Cerro Gordo, Humboldt) have NaN for the intercept and pixel coefficients, indicating insufficient data or a singularity issue in the linear model fitting for these counties.
The pixel coefficient estimates for other counties vary, with Kossuth having a coefficient of 0.1905 and Webster having 0.4259, indicating a positive relationship between pixel counts and hectares of corn. However, high Pr(>|t|) values (all > 0.05) suggest that pixel counts may not be statistically significant predictors of corn hectares in most counties. A very similar situation in the soybeans model.

2-) Fit a linear mixed model $y_{ij} =  x^t \beta + v_i + e_{ij}$ for both crops such that segments share the same countywide random effect. Make and justify the model assumptions and discuss the fits. Do they exhibit notable differences between the crops?
```{r}
mixed.lmer.corn <- lmer(HACorn ~ PixelsCorn + (1|CountyName), data = corn_data)
summary(mixed.lmer.corn)

mixed.lmer.soybeans <- lmer(HASoybeans ~ PixelsSoybeans + (1|CountyName), data = soy_data)
summary(mixed.lmer.soybeans)


```

Both models include PixelsCorn and PixelsSoybeans as predictors, respectively, which are highly significant given their t-values (8.900 for corn and 11.98 for soybeans). This indicates that the number of pixels is a strong predictor of the hectares of crops.

Here we cannot talk about a nested model since every row corresponds to a segment. Also from the equation, we can say it is a fixed slope model. Looking a variations in the random effect while explained variance for CountryName in corn segments is 62.83/(62.83 + 290.36), it is 239.2/(239.2 + 180.0). We can clearly say that country is more important in soybeans than corn. 

3-) 
In order to obtain predictions for \(\mu_i = \bar{x}_{ip}^\top \beta + v_i\), four predictors are compared and evaluated with respect to their reliability. Here, for the \(i\)-th county and a specified crop, \(\bar{x}_{ip}\) is the population mean of the explanatory variables and \(\bar{x}_i\) the mean over the observed segments only. Further, \(\hat{\beta}\) is the weighted least-squares estimator for \(\beta\) and \(\gamma_i = \frac{\sigma^2_v}{\sigma^2_v + n^{-1}_i \sigma^2_e}\),
where \(n_i\) is the number of observations in the \(i\)-th county and \(\sigma^2_v\) and \(\sigma^2_e\) are the variances of random effect and error, respectively. Also, \(\bar{y}_i = n^{-1}_i \sum_{j=1}^{n_i} y_{ij}\).

\begin{itemize}
    \item Regression predictor: \(\mu^0_i = \bar{x}_{ip}^\top \hat{\beta}\).
    \item Adjusted survey predictor: \(\mu^1_i = \bar{x}_{ip}^\top \hat{\beta} + \left( \bar{y}_i - \bar{x}_i^\top \hat{\beta} \right)\).
    \item (Empirical) BLUP: \(\mu^{\gamma_i}_i = \bar{x}_{ip}^\top \hat{\beta} + \hat{\gamma}_i \left( \bar{y}_i - \bar{x}_i^\top \hat{\beta} \right)\).
    \item Survey predictor: \(\bar{y}_i\).
\end{itemize}

An estimate for the mean squared error \(\text{MSE}_(\mu_i)( \mu^d_i) = \mathbb{E}(\mu_i - \mu^d_i)^2\) for \(\mu^d_i\) is given by
\[
\text{MSE}_(\mu_i)( \mu^d_i) = (1 - d)^2 \hat{\sigma}^2_v + \frac{d^2 \hat{\sigma}^2_e}{n_i} + 2(d - \hat{\gamma}_i)(\bar{x}_{ip} - d \bar{x}_i)^\top \hat{V}(\hat{\beta}) \bar{x}_i + (\bar{x}_{ip} - d \bar{x}_i)^\top \hat{V}(\hat{\beta}) (\bar{x}_{ip} - d \bar{x}_i).
\]

,
where $\hat{V}(\hat{\beta})$ is the covariance matrix of $\hat{\beta}$. Create a list with the predictions using each of the above predictors and print the respective MSE for each county and both crops. Discuss the results.

```{r}
extract_params <- function(model) {
  list(
    sigma_v = as.numeric(VarCorr(model)$CountyName),
    sigma_e = attr(VarCorr(model), "sc")^2,
    beta = fixef(model),
    V_beta = vcov(model)
  )
}

# Extract parameters for corn and soybeans
params_corn <- extract_params(mixed.lmer.corn)
params_soy <- extract_params(mixed.lmer.soybeans)

# Function to calculate predictors and MSEs for each county
calculate_predictors_mse <- function(data, params, predictor_col, mean_col, response_col) {
  counties <- unique(data$CountyName)
  results <- data.frame(
    CountyName = counties,
    mu_0 = numeric(length(counties)),
    mu_1 = numeric(length(counties)),
    mu_gamma = numeric(length(counties)),
    mu_bar_y = numeric(length(counties)),
    MSE_mu_0 = numeric(length(counties)),
    MSE_mu_1 = numeric(length(counties)),
    MSE_mu_gamma = numeric(length(counties))
  )
  
  for (county in counties) {
    county_data <- subset(data, CountyName == county)
    n_i <- nrow(county_data)
    bar_x_i <- mean(county_data[[predictor_col]])
    bar_x_ip <- unique(county_data[[mean_col]])
    bar_y_i <- mean(county_data[[response_col]])
    
    gamma_i <- params$sigma_v / (params$sigma_v + (params$sigma_e / n_i))
    mu_0 <- bar_x_ip * params$beta[2] + params$beta[1]
    mu_1 <- mu_0 + (bar_y_i - (bar_x_i * params$beta[2] + params$beta[1]))
    mu_gamma <- mu_0 + gamma_i * (bar_y_i - (bar_x_i * params$beta[2] + params$beta[1]))
    mu_bar_y <- bar_y_i
    
    results[results$CountyName == county, c("mu_0", "mu_1", "mu_gamma", "mu_bar_y")] <- c(mu_0, mu_1, mu_gamma, mu_bar_y)
    
    # Calculate MSEs
    x_i_vec <- c(1, bar_x_i)
    x_ip_vec <- c(1, bar_x_ip)
    
    MSE_mu_0 <- (1 - 0)^2 * params$sigma_v + (0^2 * params$sigma_e / n_i) + 
      2 * (0 - gamma_i) * t(x_ip_vec - 0 * x_i_vec) %*% params$V_beta %*% (x_ip_vec - 0 * x_i_vec) + 
      t(x_ip_vec - 0 * x_i_vec) %*% params$V_beta %*% (x_ip_vec - 0 * x_i_vec)
    
    MSE_mu_1 <- (1 - 1)^2 * params$sigma_v + (1^2 * params$sigma_e / n_i) + 
      2 * (1 - gamma_i) * t(x_ip_vec - 1 * x_i_vec) %*% params$V_beta %*% (x_ip_vec - 1 * x_i_vec) + 
      t(x_ip_vec - 1 * x_i_vec) %*% params$V_beta %*% (x_ip_vec - 1 * x_i_vec)
    
    MSE_mu_gamma <- (1 - gamma_i)^2 * params$sigma_v + (gamma_i^2 * params$sigma_e / n_i) + 
      2 * (gamma_i - gamma_i) * t(x_ip_vec - gamma_i * x_i_vec) %*% params$V_beta %*% (x_ip_vec - gamma_i * x_i_vec) + 
      t(x_ip_vec - gamma_i * x_i_vec) %*% params$V_beta %*% (x_ip_vec - gamma_i * x_i_vec)
    
    results[results$CountyName == county, c("MSE_mu_0", "MSE_mu_1", "MSE_mu_gamma")] <- c(MSE_mu_0, MSE_mu_1, MSE_mu_gamma)
  }
  return(results)
}

# Compute results for corn and soybeans
results_corn <- calculate_predictors_mse(landsat, params_corn, "PixelsCorn", "MeanPixelsCorn", "HACorn")
results_soy <- calculate_predictors_mse(landsat, params_soy, "PixelsSoybeans", "MeanPixelsSoybeans", "HASoybeans")

# Combine results for output
results <- list(corn = results_corn, soybeans = results_soy)
results

```


4-) d) Estimate the total county field size for both crops and plot the results by the BLUP from part (c) as well as the predictor only relying on the survey data in a table and onto a map of Iowa. You may use the packages ggplot2 for plotting and maps and mapdata for modelling the data frame. Comment on the results.

```{r}
iowa_map <- map_data("county", region = "iowa")


iowa_map <- iowa_map %>%
  rename(CountyName = subregion)


results$corn$CountyName <- tolower(results$corn$CountyName)
results$soybeans$CountyName <- tolower(results$soybeans$CountyName)

corn.blup.y <- results$corn %>%
  rename(mu_gamma_corn = mu_gamma, mu_bar_y_corn = mu_bar_y) %>% select(CountyName, mu_gamma_corn, mu_bar_y_corn)

soybeans.blup.y <- results$soybeans %>%
  rename(mu_gamma_soybeans = mu_gamma, mu_bar_y_soybeans = mu_bar_y) %>% select(CountyName, mu_gamma_soybeans, mu_bar_y_soybeans)


map_data <- left_join(iowa_map, corn.blup.y, by = c("CountyName"))
map_data <- left_join(map_data, soybeans.blup.y, by = c("CountyName"))


ggplot(map_data, aes(x = long, y = lat,group = group, fill = mu_gamma_soybeans)) +
  geom_polygon(color = "black") +
  scale_fill_gradient(low = "white", high = "blue", na.value = "grey50") +
  labs(fill = "BLUP", title = "BLUP of Soybeans Fields by County in Iowa")+
  theme_minimal()

ggplot(map_data, aes(x = long, y = lat,group = group, fill = mu_gamma_corn)) +
  geom_polygon(color = "black") +
  scale_fill_gradient(low = "white", high = "blue", na.value = "grey50") +
  labs(fill = "BLUP", title = "BLUP of Corn Fields by County in Iowa")+
  theme_minimal()

ggplot(map_data, aes(x = long, y = lat,group = group, fill = mu_bar_y_soybeans)) +
  geom_polygon(color = "black") +
  scale_fill_gradient(low = "white", high = "blue", na.value = "grey50") +
  labs(fill = "Hectars", title = "Hectars of Soybeans in Iowa")+
  theme_minimal()

ggplot(map_data, aes(x = long, y = lat,group = group, fill = mu_bar_y_corn)) +
  geom_polygon(color = "black") +
  scale_fill_gradient(low = "white", high = "blue", na.value = "grey50") +
  labs(fill = "Hectars", title = "Hectars of Corn in Iowa")+
  theme_minimal()

```

There seems to be a very high negative correlation between soybeans and corns! 

## Principal component analysis

#### Dataset

The dataset iris is a classical dataset in statistics, and has been already analysed in Fisher, R. A. (1936) The use of multiple measurements in taxonomic problems. Annals of Eugenics,
7, Part II, 179-188. It contains the following variables, all measured in centimeters:

-   Sepal.Length: length of the sepal
-   Sepal.Width: width of the sepal
-   Petal.Length: length of the petal
-   Petal.Width: width of the petal
-   Species: type of the iris: setosa, versicolor, or virginic

There are 50 flowers from each of the three species. All measures are in centimeters. The dataset is accessible automatically in R; try ?iris. We shall use principal component analysis for visualisation
of the data and K-means classification. In R, this can be done with the functions prcomp and kmeans.

1-) Create a reduced dataset discarding the Species. Calculate loadings and scores for the reduced dataset using the empirical covariance matrix. What proportion of total variation in the data is explained by the first two principal components? Interpret the first two principal
components.

```{r}
data(iris)

iris_reduced <- iris[, -5]

perform.pca_.firsttwo <- function(data, scale = FALSE) {

  pca_result <- prcomp(data, scale. = scale)
  
  loadings <- pca_result$rotation
  
  scores <- pca_result$x
  
  explained_variance <- summary(pca_result)$importance[2, ]
  
  proportion_variance_explained <- explained_variance[1:2]
  
  return(proportion_variance_explained)
}

print(perform.pca_.firsttwo(iris_reduced))
```
The first principal component (PC1) explains approximately 92.46% of the total variance in the dataset. The second principal component (PC2) explains approximately 5.31% of the total variance. Together, the first two principal components explain about 97.77% of the total variance in the dataset.

2-) Do the same, now using the empirical correlation matrix. Are the results similar?

```{r}
print(perform.pca_.firsttwo(iris_reduced, TRUE))
```

The first principal component (PC1) explains approximately 72.96% of the total variance. The second principal component (PC2) explains approximately 22.87% of the total variance. Together, the first two principal components explain about 95.83% of the total variance. Using the correlation matrix, the data is standardized, resulting in a more balanced explanation of variance across components compared to the covariance matrix.

3-) Create a new dataset where the petal length is measured in millimetres instead of centimetres. Have the principal components and proportion of variance explained changed when using the covariance matrix? And when using the correlation matrix?

```{r}
iris_mm <- iris[, -5]
iris_mm$Petal.Length <- iris_mm$Petal.Length * 10

print(perform.pca_.firsttwo(iris_mm))
print(perform.pca_.firsttwo(iris_mm, TRUE))
```
After transforming Petal Length to millimeters, PC1 explains 99.88% of the total variance, while PC2 explains only 0.08%. The significant change when using the covariance matrix indicates that the variance is dominated by the Petal Length, as its scale has increased tenfold. This causes the first principal component to capture almost all the variance, leaving very little for the second component.The proportions of variance explained by the first two principal components remain almost unchanged when using the correlation matrix. This is because the correlation matrix standardizes the data, making it invariant to changes in the scale of individual variables. The results are consistent regardless of whether Petal Length is measured in centimeters or millimeters.

4-) Henceforth use the original dataset (with all measurements in centimetres) and the covariance matrix. Plot the first two principal components against each other, marking the different species by colour. What do you observe? Would you expect that this two-dimensional plot of the data give a reasonable representation of the relative position of the observations in the original four-dimensional space?

```{r}
pca_cov <- prcomp(iris_reduced, scale. = FALSE)
pca_data <- data.frame(PC1 = pca_cov$x[, 1], PC2 = pca_cov$x[, 2], Species = iris$Species)

# Plot the first two principal components
ggplot(pca_data, aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 3) +
  labs(title = "PCA of Iris Dataset (Using Covariance Matrix)",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()

```
It is very informative, PC1 can clearly differetiate species from each other also given that the first two principal components capture a significant portion of the total variance.


5-) Perform K-means clustering for the first two principal components obtained in (a) with K = 3 and using set.seed(47) (the output algorithm depends quite heavily on the seed). Do clusters coincide with the iris species? What is the classification error? Note that you may need to relabel ”by hand” the result in kmeans(...)$cluster when comparing it with the variable Species. The latter needs to be converted into an integer. Is the classification error when using the entire dataset much smaller?
```{r}
set.seed(47)

# Perform K-means clustering on the first two principal components
kmeans_result <- kmeans(pca_data[, c("PC1", "PC2")], centers = 3)

# Add the cluster results to the data frame
pca_data$Cluster <- kmeans_result$cluster
species_int <- as.integer(pca_data$Species)

print(table(pca_data$Cluster, species_int))

cluster_to_species <- c(2, 3, 1)
relabeled_clusters <- cluster_to_species[pca_data$Cluster]
table(relabeled_clusters, species_int)

1-mean(relabeled_clusters != species_int)

```
The classification error is 0.5733333. Unfortunately it does not coincide much.

## Time Series

#### Dataset
Access the cmort dataset from the astsa package. The data contains the weekly cardiovascular mortality in Los Angeles County from 1970 to 1979 from following study: Shumway, R.H. (1988). Applied statistical time series analysis. Prentice-Hall, Englewood Cliffs


#### Exercises

1-) Fit an AR(2) to the data using linear regression as in the example on the Recruitment data presented in the lecture.

```{r}
data("cmort")

df <- data.frame(matrix(NA, ncol=3, nrow=(length(cmort)-2)))
names(df) <- c("x", "x_1", "x_2")

for (i in 1:(n-2)) {
  df[i, ] <- cmort[c(i, i+1, i+2)]
}

ar.model.cmort <- lm(x ~ x_1 + x_2, data=df)
summary(ar.model.cmort)
```

2-) Use the estimated coefficients from (a) to forecast the following 4 weeks together with a 95%-CI.

```{r, warning=FALSE}
coef <- ar.model.cmort$coefficients

forecasts <- numeric(4)
forecast_se <- numeric(4)
last_values <- tail(cmort, 2)

for (i in 1:4) {
  forecasts[i] <- coef[1] + coef[2] * last_values[2] + coef[3] * last_values[1]
  
  last_values <- c(last_values[2], forecasts[i])
  
  forecast_se[i] <- sqrt(sum(residuals(ar.model.cmort)^2) / (n-2))
}

alpha <- 0.05
z <- qnorm(1 - alpha / 2)
forecast_lower <- forecasts - z * forecast_se
forecast_upper <- forecasts + z * forecast_se

forecast_df <- data.frame(
  Week = (n + 1):(n + 4),
  Forecast = forecasts,
  Lower_CI = forecast_lower,
  Upper_CI = forecast_upper
)

print(forecast_df)

# Prepare data for plotting
plot_data <- data.frame(
  Week = (n-49):(n+4),
  Mortality = c(tail(cmort, 50), forecasts),
  Forecast = c(rep(NA, 50), forecasts),
  Lower_CI = c(rep(NA, 50), forecast_lower),
  Upper_CI = c(rep(NA, 50), forecast_upper)
)

# Plot the data
ggplot(plot_data, aes(x = Week, y = Mortality)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  geom_line(aes(y = Forecast), color = "red") +
  geom_point(aes(y = Forecast), color = "red") +
  geom_ribbon(aes(ymin = Lower_CI, ymax = Upper_CI), fill = "grey80", alpha = 0.5) +
  labs(title = "Forecasted Cardiovascular Mortality with 95% CI",
       x = "Week",
       y = "Mortality") +
  theme_minimal()

```


3-) Now use the Yule-Walker method to estimate the model. Compare the estimates and standard errors of the coefficients from the Yule-Walker method to the results from 1.

```{r}
acf_vals <- acf(cmort, plot = FALSE, type = "covariance")$acf[1:3]
gamma0 <- acf_vals[1]
gamma1 <- acf_vals[2]
gamma2 <- acf_vals[3]

# Yule-Walker equations
R <- matrix(c(1, gamma1/gamma0, gamma1/gamma0, 1), nrow = 2)
r <- c(gamma1/gamma0, gamma2/gamma0)
phi <- solve(R, r)
phi1_yw <- phi[1]
phi2_yw <- phi[2]

# Calculate the variance of the white noise process
sigma2_yw <- gamma0 * (1 - (phi1_yw * gamma1/gamma0 + phi2_yw * gamma2/gamma0))

# Calculate the standard errors of the coefficients
n <- length(cmort)

# Or we can use the R function ar.yw
cmort_yw <- ar.yw(cmort, order = 2)
cat("phi1 =", cmort_yw$ar[1], "\n")
cat("phi2 =", cmort_yw$ar[2], "\n")

se_phi <- sqrt(diag(cmort_yw$asy.var.coef))

# Print results
cat("Yule-Walker Estimates:\n")
cat("phi1 =", phi1_yw, "\n")
cat("phi2 =", phi2_yw, "\n")
cat("Standard error:\n")
cat("se_phi =", se_phi, "\n")

coef_lm <- ar.model.cmort$coefficients
se_lm <- summary(ar.model.cmort)$coefficients[, "Std. Error"]

cat("\nLinear Regression Estimates (from part a):\n")
cat("phi1 =", coef_lm[2], "\n")
cat("phi2 =", coef_lm[3], "\n")
cat("Standard errors:\n")
cat("se_phi1 =", se_lm[2], "\n")
cat("se_phi2 =", se_lm[3], "\n")
```


4-) Predict the following 4 weeks using the estimations from the Yule-Walker method.
5-) Compare the estimated standard errors of the coefficients obtained by linear regression with their corresponding asymptotic approximations as given by the asymptotic distribution of the estimators on the slides.

```{r}
forecasts_yw <- numeric(4)
last_values_yw <- tail(cmort, 2)

# Perform the forecasts
for (i in 1:4) {
  forecasts_yw[i] <- phi1_yw * last_values_yw[2] + phi2_yw * last_values_yw[1]
  last_values_yw <- c(last_values_yw[2], forecasts_yw[i])
}

forecast_lower_yw <- forecasts_yw - z * se_phi
forecast_upper_yw <- forecasts_yw + z * se_phi

# Create the forecast dataframe for Yule-Walker
forecast_df_yw <- data.frame(
  Week = (n + 1):(n + 4),
  Forecast = forecasts_yw,
  Lower_CI = forecast_lower_yw,
  Upper_CI = forecast_upper_yw
)

print(forecast_df_yw)


# Calculate asymptotic variance-covariance matrix
asymptotic_var_cov_matrix <- (solve(R) * sigma2_yw / n) / gamma0
se_phi_yw_asymptotic <- sqrt(asymptotic_var_cov_matrix[1, 1])

cat("se_phi_yw_asymptotic =", se_phi_yw_asymptotic, "\n")

```

The estimated standard errors from linear regression (0.03998 and 0.04007) are very close to the Yule-Walker standard errors (0.04001 and 0.04001).

6-) Try to fit an ARMA(2,2) model to the data. Does the more complex model provides a better fit or is an AR(2) model enough?

```{r}

ar.ols(cmort, order=2, demean=FALSE, intercept=TRUE)
arima(cmort, order = c(2, 0, 2))

```

The ARMA model results in different estimates and larger standard errors due to the inclusion of moving average terms, highlighting how additional parameters can influence the estimates and their precision. However, the moving average coefficients are close to zero, indicating that they do not significantly contribute to the estimation.

## References

1. McCullagh, P., \& Nelder, J. A. (1989). *Generalized Linear Models*. Chapman and Hall/CRC.
2. Shumway, R. H. (2000). *Time Series Analysis and Its Applications*. Springer.
3. Finding the maximum likelihood estimator. Retrieved from [Math Stack Exchange](https://math.stackexchange.com/questions/240496/finding-the-maximum-likelihood-estimator).
4. Condition number. Retrieved from [Illinois Engineering](https://courses.engr.illinois.edu/cs357/sp2024/notes/ref-10-condition.html).
5. Cause of a high condition number in a Python statsmodels regression. Retrieved from [Stack Exchange](https://stats.stackexchange.com/questions/243000/cause-of-a-high-condition-number-in-a-python-statsmodels-regression#comment462188_243000).

